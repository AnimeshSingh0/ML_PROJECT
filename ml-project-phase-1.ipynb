{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-08T09:35:21.327010Z","iopub.execute_input":"2023-11-08T09:35:21.327292Z","iopub.status.idle":"2023-11-08T09:35:21.332705Z","shell.execute_reply.started":"2023-11-08T09:35:21.327266Z","shell.execute_reply":"2023-11-08T09:35:21.331780Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# LOAD NECESSARY LIBRARIES","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sn\n\nfrom tensorflow import keras\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split, GroupShuffleSplit \n\nimport glob\nimport sys\nimport os\nimport math\nimport gc\nimport sys\nimport sklearn\nimport scipy\n\nprint(f'Tensorflow V{tf.__version__}')\nprint(f'Keras V{tf.keras.__version__}')\nprint(f'Python V{sys.version}')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-08T09:35:21.379658Z","iopub.execute_input":"2023-11-08T09:35:21.379949Z","iopub.status.idle":"2023-11-08T09:35:29.249704Z","shell.execute_reply.started":"2023-11-08T09:35:21.379926Z","shell.execute_reply":"2023-11-08T09:35:29.248769Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n\nTensorFlow Addons (TFA) has ended development and introduction of new features.\nTFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\nPlease modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n\nFor more information see: https://github.com/tensorflow/addons/issues/2807 \n\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Tensorflow V2.12.0\nKeras V2.12.0\nPython V3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:40:32) [GCC 12.3.0]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# PARAMETERS","metadata":{}},{"cell_type":"code","source":"# PREPROCESS_DATA = False\n# TRAIN_MODEL = True\n# USE_VAL = False\n\nN_ROWS = 543\nN_DIMS = 3\nDIM_NAMES = ['x', 'y', 'z']\nSEED = 42\nNUM_CLASSES = 250\n# IS_INTERACTIVE = os.environ['KAGGLE_KERNEL_RUN_TYPE'] == 'Interactive'\n# VERBOSE = 1 if IS_INTERACTIVE else 2\n\nINPUT_SIZE = 12\nBATCH_ALL_SIGNS_N = 1\nBATCH_SIZE = 256\nN_EPOCHS = 120\nLR_MAX = 0.001\nN_WARMUP_EPOCHS = 0\nWD_RATIO = 0.05\nMASK_VAL = 4237\n\n# N_MODELS = 4 # WAS 1\n# DEBUG = False","metadata":{"execution":{"iopub.status.busy":"2023-11-08T09:35:29.251503Z","iopub.execute_input":"2023-11-08T09:35:29.252154Z","iopub.status.idle":"2023-11-08T09:35:29.259933Z","shell.execute_reply.started":"2023-11-08T09:35:29.252118Z","shell.execute_reply":"2023-11-08T09:35:29.257281Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Prints Shape and Dtype For List Of Variables\ndef print_shape_dtype(l, names):\n    for e, n in zip(l, names):\n        print(f'{n} shape: {e.shape}, dtype: {e.dtype}')","metadata":{"execution":{"iopub.status.busy":"2023-11-08T09:35:29.262666Z","iopub.execute_input":"2023-11-08T09:35:29.263286Z","iopub.status.idle":"2023-11-08T09:35:29.271756Z","shell.execute_reply.started":"2023-11-08T09:35:29.263256Z","shell.execute_reply":"2023-11-08T09:35:29.267637Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"USE_TYPES = ['left_hand', 'pose', 'right_hand']\nSTART_IDX = 468\nLIPS_IDXS0 = np.array([\n        61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n        291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n        78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n        95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n    ])\nLEFT_HAND_IDXS0 = np.arange(468,489)\nRIGHT_HAND_IDXS0 = np.arange(522,543)\nLEFT_POSE_IDXS0 = np.array([502, 504, 506, 508, 510])\nRIGHT_POSE_IDXS0 = np.array([503, 505, 507, 509, 511])\nLANDMARK_IDXS_LEFT_DOMINANT0 = np.concatenate((LIPS_IDXS0, LEFT_HAND_IDXS0, LEFT_POSE_IDXS0))\nLANDMARK_IDXS_RIGHT_DOMINANT0 = np.concatenate((LIPS_IDXS0, RIGHT_HAND_IDXS0, RIGHT_POSE_IDXS0))\nHAND_IDXS0 = np.concatenate((LEFT_HAND_IDXS0, RIGHT_HAND_IDXS0), axis=0)\nN_COLS = LANDMARK_IDXS_LEFT_DOMINANT0.size\nLIPS_IDXS = np.argwhere(np.isin(LANDMARK_IDXS_LEFT_DOMINANT0, LIPS_IDXS0)).squeeze()\nLEFT_HAND_IDXS = np.argwhere(np.isin(LANDMARK_IDXS_LEFT_DOMINANT0, LEFT_HAND_IDXS0)).squeeze()\nRIGHT_HAND_IDXS = np.argwhere(np.isin(LANDMARK_IDXS_LEFT_DOMINANT0, RIGHT_HAND_IDXS0)).squeeze()\nHAND_IDXS = np.argwhere(np.isin(LANDMARK_IDXS_LEFT_DOMINANT0, HAND_IDXS0)).squeeze()\nPOSE_IDXS = np.argwhere(np.isin(LANDMARK_IDXS_LEFT_DOMINANT0, LEFT_POSE_IDXS0)).squeeze()\nprint(f'# HAND_IDXS: {len(HAND_IDXS)}, N_COLS: {N_COLS}')\nLIPS_START=0","metadata":{"execution":{"iopub.status.busy":"2023-11-08T09:35:29.276672Z","iopub.execute_input":"2023-11-08T09:35:29.277015Z","iopub.status.idle":"2023-11-08T09:35:29.292250Z","shell.execute_reply.started":"2023-11-08T09:35:29.276992Z","shell.execute_reply":"2023-11-08T09:35:29.291252Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"# HAND_IDXS: 21, N_COLS: 66\n","output_type":"stream"}]},{"cell_type":"code","source":"ROWS_PER_FRAME = 543  \n\ndef load_relevant_data_subset(pq_path):\n    data_columns = ['x', 'y', 'z']\n    data = pd.read_parquet(pq_path, columns=data_columns)\n    n_frames = int(len(data) / ROWS_PER_FRAME)\n    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n    return data.astype(np.float32)","metadata":{"execution":{"iopub.status.busy":"2023-11-08T09:35:29.293604Z","iopub.execute_input":"2023-11-08T09:35:29.293895Z","iopub.status.idle":"2023-11-08T09:35:29.299226Z","shell.execute_reply.started":"2023-11-08T09:35:29.293873Z","shell.execute_reply":"2023-11-08T09:35:29.298282Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# PREPROCESSING","metadata":{}},{"cell_type":"markdown","source":"First, the layer creates a constant called normalisation_correction in the __init__ method. This constant is a matrix with the number of rows equal to the number of landmark points of a particular type in the data and the number of columns being 3 (that is, the x, y, and z coordinates). This matrix is used to correct the camera's shooting direction, adjusting the left hand to the right hand and the right hand to the left hand.\n\nThis layer also defines a method called pad_edge that is used to pad a given tensor with a certain number of repeating elements on the left or right side. Next, the layer uses the @tf.function decorator to decorate a call method that handles the input data.\n\nThis method first calculates the number of frames of the input data (N_FRAMES0), and then finds the landmark point of the dominant hand in the data by calculating the sum of the coordinates of the left and right hands in the data. Next, the method counts the number of non-NaN values in the dominant hand for each frame to determine which frames need to be retained. It then uses these indexes to collect landmark data from the input data.\n\nThe method next converts the data type of the frame index from an integer to a floating point number and then normalizes it to start with 0. Next, it again counts the number of frames (N_FRAMES) of the filtered data, and then collects specific types of landmark data from these data. If the number of frames of data is smaller than the specified input size (INPUT_SIZE), it is padded with -1, extends the number of frames of data to the specified input size, and replaces NaN values with 0. If the number of frames of data is larger than the specified input size, it is reduced to the specified input size using duplicate data and any missing data is filled in.\n\nFinally, the method returns the processed data and corresponding frame index.","metadata":{}},{"cell_type":"code","source":"class PreprocessLayer(tf.keras.layers.Layer):\n    def __init__(self):\n        super(PreprocessLayer, self).__init__()\n        normalisation_correction = tf.constant([\n                    # Add 0.50 to left hand (original right hand) and substract 0.50 of right hand (original left hand)\n                    [0] * len(LIPS_IDXS) + [0.50] * len(LEFT_HAND_IDXS) + [0.50] * len(POSE_IDXS),\n                    # Y coordinates stay intact\n                    [0] * len(LANDMARK_IDXS_LEFT_DOMINANT0),\n                    # Z coordinates stay intact\n                    [0] * len(LANDMARK_IDXS_LEFT_DOMINANT0),\n                ],\n                dtype=tf.float32,\n            )\n        self.normalisation_correction = tf.transpose(normalisation_correction, [1,0])\n        \n    def pad_edge(self, t, repeats, side):\n        if side == 'LEFT':\n            return tf.concat((tf.repeat(t[:1], repeats=repeats, axis=0), t), axis=0)\n        elif side == 'RIGHT':\n            return tf.concat((t, tf.repeat(t[-1:], repeats=repeats, axis=0)), axis=0)\n    \n    @tf.function(\n        input_signature=(tf.TensorSpec(shape=[None,N_ROWS,N_DIMS], dtype=tf.float32),),\n    )\n    def call(self, data0):\n        \n        # TRUNCATE LONG VIDEOS\n        N_FRAMES0 = tf.shape(data0)[0]\n        data0 = tf.slice(data0, [0,0,0], [tf.math.minimum(INPUT_SIZE * INPUT_SIZE,N_FRAMES0),-1,-1])\n        \n        # Number of Frames in Video\n        N_FRAMES0 = tf.shape(data0)[0]\n        \n        # Find dominant hand by comparing summed absolute coordinates\n        left_hand_sum = tf.math.reduce_sum(tf.where(tf.math.is_nan(tf.gather(data0, LEFT_HAND_IDXS0, axis=1)), 0, 1))\n        right_hand_sum = tf.math.reduce_sum(tf.where(tf.math.is_nan(tf.gather(data0, RIGHT_HAND_IDXS0, axis=1)), 0, 1))\n        left_dominant = left_hand_sum >= right_hand_sum\n        \n        # Count non NaN Hand values in each frame for the dominant hand\n        if left_dominant:\n            frames_hands_non_nan_sum = tf.math.reduce_sum(\n                    tf.where(tf.math.is_nan(tf.gather(data0, LEFT_HAND_IDXS0, axis=1)), 0, 1),\n                    axis=[1, 2],\n                )\n        else:\n            frames_hands_non_nan_sum = tf.math.reduce_sum(\n                    tf.where(tf.math.is_nan(tf.gather(data0, RIGHT_HAND_IDXS0, axis=1)), 0, 1),\n                    axis=[1, 2],\n                )\n        \n        # Find frames indices with coordinates of dominant hand\n        non_empty_frames_idxs = tf.where(frames_hands_non_nan_sum > 0)\n        non_empty_frames_idxs = tf.squeeze(non_empty_frames_idxs, axis=1)\n        # Filter frames\n        data = tf.gather(data0, non_empty_frames_idxs, axis=0)\n        \n        # Cast Indices in float32 to be compatible with Tensorflow Lite\n        non_empty_frames_idxs = tf.cast(non_empty_frames_idxs, tf.float32)\n        # Normalize to start with 0\n        non_empty_frames_idxs -= tf.reduce_min(non_empty_frames_idxs)\n        \n        # Number of Frames in Filtered Video\n        N_FRAMES = tf.shape(data)[0]\n        \n        # Gather Relevant Landmark Columns\n        if left_dominant:\n            data = tf.gather(data, LANDMARK_IDXS_LEFT_DOMINANT0, axis=1)\n        else:\n            data = tf.gather(data, LANDMARK_IDXS_RIGHT_DOMINANT0, axis=1)\n            data = (\n                    self.normalisation_correction + (\n                        (data - self.normalisation_correction) * tf.where(self.normalisation_correction != 0, -1.0, 1.0))\n                )\n        \n        # Video fits in INPUT_SIZE\n        if N_FRAMES < INPUT_SIZE:\n            # Pad With -1 to indicate padding\n            non_empty_frames_idxs = tf.pad(non_empty_frames_idxs, [[0, INPUT_SIZE-N_FRAMES]], constant_values=-1)\n            # Pad Data With Zeros\n            data = tf.pad(data, [[0, INPUT_SIZE-N_FRAMES], [0,0], [0,0]], constant_values=0)\n            # Fill NaN Values With 0\n            data = tf.where(tf.math.is_nan(data), 0.0, data)\n            return data, non_empty_frames_idxs\n        # Video needs to be downsampled to INPUT_SIZE\n        else:\n            # Repeat\n            if N_FRAMES < INPUT_SIZE**2:\n                repeats = tf.math.floordiv(INPUT_SIZE * INPUT_SIZE, N_FRAMES0)\n                data = tf.repeat(data, repeats=repeats, axis=0)\n                non_empty_frames_idxs = tf.repeat(non_empty_frames_idxs, repeats=repeats, axis=0)\n\n            # Pad To Multiple Of Input Size\n            pool_size = tf.math.floordiv(len(data), INPUT_SIZE)\n            if tf.math.mod(len(data), INPUT_SIZE) > 0:\n                pool_size += 1\n\n            if pool_size == 1:\n                pad_size = (pool_size * INPUT_SIZE) - len(data)\n            else:\n                pad_size = (pool_size * INPUT_SIZE) % len(data)\n\n            # Pad Start/End with Start/End value\n            pad_left = tf.math.floordiv(pad_size, 2) + tf.math.floordiv(INPUT_SIZE, 2)\n            pad_right = tf.math.floordiv(pad_size, 2) + tf.math.floordiv(INPUT_SIZE, 2)\n            if tf.math.mod(pad_size, 2) > 0:\n                pad_right += 1\n\n            # Pad By Concatenating Left/Right Edge Values\n            data = self.pad_edge(data, pad_left, 'LEFT')\n            data = self.pad_edge(data, pad_right, 'RIGHT')\n\n            # Pad Non Empty Frame Indices\n            non_empty_frames_idxs = self.pad_edge(non_empty_frames_idxs, pad_left, 'LEFT')\n            non_empty_frames_idxs = self.pad_edge(non_empty_frames_idxs, pad_right, 'RIGHT')\n\n            # Reshape to Mean Pool\n            data = tf.reshape(data, [INPUT_SIZE, -1, N_COLS, N_DIMS])\n            non_empty_frames_idxs = tf.reshape(non_empty_frames_idxs, [INPUT_SIZE, -1])\n\n            # Mean Pool\n            data = tf.experimental.numpy.nanmean(data, axis=1)\n            non_empty_frames_idxs = tf.experimental.numpy.nanmean(non_empty_frames_idxs, axis=1)\n\n            # Fill NaN Values With 0\n            data = tf.where(tf.math.is_nan(data), 0.0, data)\n            \n            return data, non_empty_frames_idxs\n    \npreprocess_layer = PreprocessLayer()","metadata":{"execution":{"iopub.status.busy":"2023-11-08T09:35:29.300826Z","iopub.execute_input":"2023-11-08T09:35:29.301174Z","iopub.status.idle":"2023-11-08T09:35:32.078397Z","shell.execute_reply.started":"2023-11-08T09:35:29.301138Z","shell.execute_reply":"2023-11-08T09:35:32.077375Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def get_data(file_path):\n    # Load Raw Data\n    data = load_relevant_data_subset(file_path)\n    # Process Data Using Tensorflow\n    data = preprocess_layer(data)\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2023-11-08T09:35:32.079985Z","iopub.execute_input":"2023-11-08T09:35:32.080279Z","iopub.status.idle":"2023-11-08T09:35:32.084671Z","shell.execute_reply.started":"2023-11-08T09:35:32.080254Z","shell.execute_reply":"2023-11-08T09:35:32.083693Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def preprocess_data():\n    # Create arrays to save data\n    X = np.zeros([N_SAMPLES, INPUT_SIZE, N_COLS, N_DIMS], dtype=np.float32)\n    y = np.zeros([N_SAMPLES], dtype=np.int32)\n    NON_EMPTY_FRAME_IDXS = np.full([N_SAMPLES, INPUT_SIZE], -1, dtype=np.float32)\n\n    # Fill X/y\n    for row_idx, (file_path, sign_ord) in enumerate(tqdm(train[['file_path', 'sign_ord']].values)):\n        # Log message every 5000 samples\n        if row_idx % 5000 == 0:\n            print(f'Generated {row_idx}/{N_SAMPLES}')\n\n        data, non_empty_frame_idxs = get_data(file_path)\n        X[row_idx] = data\n        y[row_idx] = sign_ord\n        NON_EMPTY_FRAME_IDXS[row_idx] = non_empty_frame_idxs\n        # Sanity check, data should not contain NaN values\n        if np.isnan(data).sum() > 0:\n            print(row_idx)\n            return data\n\n    # Save X/y\n    np.save(ROOT_DIR+'/X.npy', X)\n    np.save(ROOT_DIR+'/y.npy', y)\n    np.save(ROOT_DIR+'/NON_EMPTY_FRAME_IDXS.npy', NON_EMPTY_FRAME_IDXS)\n    \n    # Save Validation\n    splitter = GroupShuffleSplit(test_size=0.10, n_splits=2, random_state=SEED)\n    PARTICIPANT_IDS = train['participant_id'].values\n    train_idxs, val_idxs = next(splitter.split(X, y, groups=PARTICIPANT_IDS))\n\n    # Save Train\n    X_train = X[train_idxs]\n    NON_EMPTY_FRAME_IDXS_TRAIN = NON_EMPTY_FRAME_IDXS[train_idxs]\n    y_train = y[train_idxs]\n    np.save(ROOT_DIR+'/X_train.npy', X_train)\n    np.save(ROOT_DIR+'/y_train.npy', y_train)\n    np.save(ROOT_DIR+'/NON_EMPTY_FRAME_IDXS_TRAIN.npy', NON_EMPTY_FRAME_IDXS_TRAIN)\n    # Save Validation\n    X_val = X[val_idxs]\n    NON_EMPTY_FRAME_IDXS_VAL = NON_EMPTY_FRAME_IDXS[val_idxs]\n    y_val = y[val_idxs]\n    np.save(ROOT_DIR+'/X_val.npy', X_val)\n    np.save(ROOT_DIR+'/y_val.npy', y_val)\n    np.save(ROOT_DIR+'/NON_EMPTY_FRAME_IDXS_VAL.npy', NON_EMPTY_FRAME_IDXS_VAL)\n    # Split Statistics\n    print(f'Patient ID Intersection Train/Val: {set(PARTICIPANT_IDS[train_idxs]).intersection(PARTICIPANT_IDS[val_idxs])}')\n    print(f'X_train shape: {X_train.shape}, X_val shape: {X_val.shape}')\n    print(f'y_train shape: {y_train.shape}, y_val shape: {y_val.shape}')","metadata":{"execution":{"iopub.status.busy":"2023-11-08T09:35:32.085911Z","iopub.execute_input":"2023-11-08T09:35:32.086257Z","iopub.status.idle":"2023-11-08T09:35:32.099951Z","shell.execute_reply.started":"2023-11-08T09:35:32.086233Z","shell.execute_reply":"2023-11-08T09:35:32.098869Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/asl-signs/train.csv')\nN_SAMPLES = len(train)\nprint(f'N_SAMPLES: {N_SAMPLES}')","metadata":{"execution":{"iopub.status.busy":"2023-11-08T09:35:32.101021Z","iopub.execute_input":"2023-11-08T09:35:32.101309Z","iopub.status.idle":"2023-11-08T09:35:32.309948Z","shell.execute_reply.started":"2023-11-08T09:35:32.101286Z","shell.execute_reply":"2023-11-08T09:35:32.308969Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"N_SAMPLES: 94477\n","output_type":"stream"}]},{"cell_type":"code","source":"# Get complete file path to file\ndef get_file_path(path):\n    return f'/kaggle/input/asl-signs/{path}'\n\ntrain['file_path'] = train['path'].apply(get_file_path)\ntrain['sign_ord'] = train['sign'].astype('category').cat.codes\nORD2SIGN = train[['sign_ord', 'sign']].set_index('sign_ord').squeeze().to_dict()","metadata":{"execution":{"iopub.status.busy":"2023-11-08T09:35:32.314018Z","iopub.execute_input":"2023-11-08T09:35:32.314595Z","iopub.status.idle":"2023-11-08T09:35:32.502120Z","shell.execute_reply.started":"2023-11-08T09:35:32.314563Z","shell.execute_reply":"2023-11-08T09:35:32.501155Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"ROOT_DIR = 'data1'\nos.mkdir(ROOT_DIR)\npreprocess_data()\n    \n# Load Train\nX_train = np.load(f'{ROOT_DIR}/X_train.npy')\ny_train = np.load(f'{ROOT_DIR}/y_train.npy')\nNON_EMPTY_FRAME_IDXS_TRAIN = np.load(f'{ROOT_DIR}/NON_EMPTY_FRAME_IDXS_TRAIN.npy')\n# Load Val\nX_val = np.load(f'{ROOT_DIR}/X_val.npy')\ny_val = np.load(f'{ROOT_DIR}/y_val.npy')\nNON_EMPTY_FRAME_IDXS_VAL = np.load(f'{ROOT_DIR}/NON_EMPTY_FRAME_IDXS_VAL.npy')\n# Define validation Data\nvalidation_data = ({ 'frames': X_val, 'non_empty_frame_idxs': NON_EMPTY_FRAME_IDXS_VAL }, y_val)\n\n# Train \nprint_shape_dtype([X_train, y_train, NON_EMPTY_FRAME_IDXS_TRAIN], ['X_train', 'y_train', 'NON_EMPTY_FRAME_IDXS_TRAIN'])\n# Val\nprint_shape_dtype([X_val, y_val, NON_EMPTY_FRAME_IDXS_VAL], ['X_val', 'y_val', 'NON_EMPTY_FRAME_IDXS_VAL'])\n# Sanity Check\nprint(f'# NaN Values X_train: {np.isnan(X_train).sum()}')","metadata":{"execution":{"iopub.status.busy":"2023-11-08T09:35:32.503344Z","iopub.execute_input":"2023-11-08T09:35:32.503629Z","iopub.status.idle":"2023-11-08T10:11:59.778986Z","shell.execute_reply.started":"2023-11-08T09:35:32.503605Z","shell.execute_reply":"2023-11-08T10:11:59.778010Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/94477 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0db86ddfbaf4bacb1a8ae707c2df994"}},"metadata":{}},{"name":"stdout","text":"Generated 0/94477\nGenerated 5000/94477\nGenerated 10000/94477\nGenerated 15000/94477\nGenerated 20000/94477\nGenerated 25000/94477\nGenerated 30000/94477\nGenerated 35000/94477\nGenerated 40000/94477\nGenerated 45000/94477\nGenerated 50000/94477\nGenerated 55000/94477\nGenerated 60000/94477\nGenerated 65000/94477\nGenerated 70000/94477\nGenerated 75000/94477\nGenerated 80000/94477\nGenerated 85000/94477\nGenerated 90000/94477\nPatient ID Intersection Train/Val: set()\nX_train shape: (80229, 12, 66, 3), X_val shape: (14248, 12, 66, 3)\ny_train shape: (80229,), y_val shape: (14248,)\nX_train shape: (80229, 12, 66, 3), dtype: float32\ny_train shape: (80229,), dtype: int32\nNON_EMPTY_FRAME_IDXS_TRAIN shape: (80229, 12), dtype: float32\nX_val shape: (14248, 12, 66, 3), dtype: float32\ny_val shape: (14248,), dtype: int32\nNON_EMPTY_FRAME_IDXS_VAL shape: (14248, 12), dtype: float32\n# NaN Values X_train: 0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# LANDMARK MEAN AND STDS","metadata":{}},{"cell_type":"code","source":"def get_lips_mean_std():\n    # LIPS\n    LIPS_MEAN_X = np.zeros([LIPS_IDXS.size], dtype=np.float32)\n    LIPS_MEAN_Y = np.zeros([LIPS_IDXS.size], dtype=np.float32)\n    LIPS_STD_X = np.zeros([LIPS_IDXS.size], dtype=np.float32)\n    LIPS_STD_Y = np.zeros([LIPS_IDXS.size], dtype=np.float32)\n\n    #fig, axes = plt.subplots(3, 1, figsize=(15, N_DIMS*6))\n\n    for col, ll in enumerate(tqdm( np.transpose(X_train[:,:,LIPS_IDXS], [2,3,0,1]).reshape([LIPS_IDXS.size, N_DIMS, -1]) )):\n        for dim, l in enumerate(ll):\n            v = l[np.nonzero(l)]\n            if dim == 0: # X\n                LIPS_MEAN_X[col] = v.mean()\n                LIPS_STD_X[col] = v.std()\n            if dim == 1: # Y\n                LIPS_MEAN_Y[col] = v.mean()\n                LIPS_STD_Y[col] = v.std()\n\n            #axes[dim].boxplot(v, notch=False, showfliers=False, positions=[col], whis=[5,95])\n\n    #for ax, dim_name in zip(axes, DIM_NAMES):\n    #    ax.set_title(f'Lips {dim_name.upper()} Dimension', size=24)\n    #    ax.tick_params(axis='x', labelsize=8)\n    #    ax.grid(axis='y')\n\n    #plt.subplots_adjust(hspace=0.50)\n    #plt.show()\n\n    LIPS_MEAN = np.array([LIPS_MEAN_X, LIPS_MEAN_Y]).T\n    LIPS_STD = np.array([LIPS_STD_X, LIPS_STD_Y]).T\n    \n    return LIPS_MEAN, LIPS_STD\n\nLIPS_MEAN, LIPS_STD = get_lips_mean_std()","metadata":{"execution":{"iopub.status.busy":"2023-11-08T10:12:10.651364Z","iopub.execute_input":"2023-11-08T10:12:10.652171Z","iopub.status.idle":"2023-11-08T10:12:13.068363Z","shell.execute_reply.started":"2023-11-08T10:12:10.652141Z","shell.execute_reply":"2023-11-08T10:12:13.067419Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/40 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e178ecfd4d5b4a1aa3fef30bfb2fab5f"}},"metadata":{}}]},{"cell_type":"code","source":"def get_left_right_hand_mean_std():\n    # LEFT HAND\n    LEFT_HANDS_MEAN_X = np.zeros([LEFT_HAND_IDXS.size], dtype=np.float32)\n    LEFT_HANDS_MEAN_Y = np.zeros([LEFT_HAND_IDXS.size], dtype=np.float32)\n    LEFT_HANDS_STD_X = np.zeros([LEFT_HAND_IDXS.size], dtype=np.float32)\n    LEFT_HANDS_STD_Y = np.zeros([LEFT_HAND_IDXS.size], dtype=np.float32)\n\n    #fig, axes = plt.subplots(3, 1, figsize=(15, N_DIMS*6))\n\n    for col, ll in enumerate(tqdm( np.transpose(X_train[:,:,LEFT_HAND_IDXS], [2,3,0,1]).reshape([LEFT_HAND_IDXS.size, N_DIMS, -1]) )):\n        for dim, l in enumerate(ll):\n            v = l[np.nonzero(l)]\n            if dim == 0: # X\n                LEFT_HANDS_MEAN_X[col] = v.mean()\n                LEFT_HANDS_STD_X[col] = v.std()\n            if dim == 1: # Y\n                LEFT_HANDS_MEAN_Y[col] = v.mean()\n                LEFT_HANDS_STD_Y[col] = v.std()\n            # Plot\n            #axes[dim].boxplot(v, notch=False, showfliers=False, positions=[col], whis=[5,95])\n\n    #for ax, dim_name in zip(axes, DIM_NAMES):\n    #    ax.set_title(f'Hands {dim_name.upper()} Dimension', size=24)\n    #    ax.tick_params(axis='x', labelsize=8)\n    #    ax.grid(axis='y')\n\n    #plt.subplots_adjust(hspace=0.50)\n    #plt.show()\n\n    LEFT_HANDS_MEAN = np.array([LEFT_HANDS_MEAN_X, LEFT_HANDS_MEAN_Y]).T\n    LEFT_HANDS_STD = np.array([LEFT_HANDS_STD_X, LEFT_HANDS_STD_Y]).T\n    \n    return LEFT_HANDS_MEAN, LEFT_HANDS_STD\n\nLEFT_HANDS_MEAN, LEFT_HANDS_STD = get_left_right_hand_mean_std()","metadata":{"execution":{"iopub.status.busy":"2023-11-08T10:12:18.575612Z","iopub.execute_input":"2023-11-08T10:12:18.576652Z","iopub.status.idle":"2023-11-08T10:12:19.817511Z","shell.execute_reply.started":"2023-11-08T10:12:18.576618Z","shell.execute_reply":"2023-11-08T10:12:19.816432Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/21 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02c604e58d21446ba9195d3d7f61e726"}},"metadata":{}}]},{"cell_type":"code","source":"def get_pose_mean_std():\n    # POSE\n    POSE_MEAN_X = np.zeros([POSE_IDXS.size], dtype=np.float32)\n    POSE_MEAN_Y = np.zeros([POSE_IDXS.size], dtype=np.float32)\n    POSE_STD_X = np.zeros([POSE_IDXS.size], dtype=np.float32)\n    POSE_STD_Y = np.zeros([POSE_IDXS.size], dtype=np.float32)\n\n    #fig, axes = plt.subplots(3, 1, figsize=(15, N_DIMS*6))\n\n    for col, ll in enumerate(tqdm( np.transpose(X_train[:,:,POSE_IDXS], [2,3,0,1]).reshape([POSE_IDXS.size, N_DIMS, -1]) )):\n        for dim, l in enumerate(ll):\n            v = l[np.nonzero(l)]\n            if dim == 0: # X\n                POSE_MEAN_X[col] = v.mean()\n                POSE_STD_X[col] = v.std()\n            if dim == 1: # Y\n                POSE_MEAN_Y[col] = v.mean()\n                POSE_STD_Y[col] = v.std()\n\n            #axes[dim].boxplot(v, notch=False, showfliers=False, positions=[col], whis=[5,95])\n\n    #for ax, dim_name in zip(axes, DIM_NAMES):\n    #    ax.set_title(f'Pose {dim_name.upper()} Dimension', size=24)\n    #    ax.tick_params(axis='x', labelsize=8)\n    #    ax.grid(axis='y')\n\n    #plt.subplots_adjust(hspace=0.50)\n    #plt.show()\n\n    POSE_MEAN = np.array([POSE_MEAN_X, POSE_MEAN_Y]).T\n    POSE_STD = np.array([POSE_STD_X, POSE_STD_Y]).T\n    \n    return POSE_MEAN, POSE_STD\n\nPOSE_MEAN, POSE_STD = get_pose_mean_std()","metadata":{"execution":{"iopub.status.busy":"2023-11-08T10:13:18.712375Z","iopub.execute_input":"2023-11-08T10:13:18.712921Z","iopub.status.idle":"2023-11-08T10:13:18.997795Z","shell.execute_reply.started":"2023-11-08T10:13:18.712891Z","shell.execute_reply":"2023-11-08T10:13:18.996864Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6b498de7a61404989c81e7076837caa"}},"metadata":{}}]},{"cell_type":"markdown","source":"# DATA LOADER WITH AUGMENTATION","metadata":{}},{"cell_type":"code","source":"TIME_AUG_PROB = 0.25\nFRAME_DROP_PROB = 0.0\n\n# Custom sampler to get a batch containing N times all signs\ndef get_train_batch_all_signs(X, y, NON_EMPTY_FRAME_IDXS, n=BATCH_ALL_SIGNS_N):\n    # Arrays to store batch in\n    X_batch = np.zeros([NUM_CLASSES*n, INPUT_SIZE, N_COLS, N_DIMS], dtype=np.float32)\n    y_batch = np.arange(0, NUM_CLASSES, step=1/n, dtype=np.float32).astype(np.int64)\n    non_empty_frame_idxs_batch = np.zeros([NUM_CLASSES*n, INPUT_SIZE], dtype=np.float32)\n    \n    # Dictionary mapping ordinally encoded sign to corresponding sample indices\n    CLASS2IDXS = {}\n    for i in range(NUM_CLASSES):\n        CLASS2IDXS[i] = np.argwhere(y == i).squeeze().astype(np.int32)\n            \n    while True:\n        # Fill batch arrays\n        for i in range(NUM_CLASSES):\n            idxs = np.random.choice(CLASS2IDXS[i], n)\n            tmp = X[idxs].copy()\n            tmp2 = NON_EMPTY_FRAME_IDXS[idxs].copy()\n            \n            # FRAME DROP AUGMENTATION\n            if np.random.uniform(0,1)<FRAME_DROP_PROB:\n                j = np.random.randint(0,INPUT_SIZE)\n                tmp[0,j,:,:] = 0\n            \n            # TIME SCALE AUGMENTATION\n            if np.random.uniform(0,1)<TIME_AUG_PROB:\n                ct = ( NON_EMPTY_FRAME_IDXS[idxs[0]] != -1 ).sum()\n                if (ct==12):\n                    mask = np.random.choice([True, False],12)\n                    mask[0] = True\n                    mask[-1] = True\n                    c = mask.sum()\n                    tmp[0,:c,:,:] = tmp[0,mask,:,:]\n                    tmp[0,c:,:,:] = 0\n                    tmp2[0,:c] = tmp2[0,mask]\n                    tmp2[0,c:] = -1\n                elif ct>6:\n                    tmp[0,:6,:,:] = tmp[0,::2,:,:]\n                    tmp[0,6:,:,:] = 0\n                    tmp2[0,:6] = tmp2[0,::2]\n                    tmp2[0,6:] = -1\n                elif (ct<=2)&(np.random.uniform(0,1)<0.3):\n                    tmp[0,::6,:,:] = tmp[0,:2,:,:]\n                    tmp[0,1::6,:,:] = tmp[0,:2,:,:]\n                    tmp[0,2::6,:,:] = tmp[0,:2,:,:]\n                    tmp[0,3::6,:,:] = tmp[0,:2,:,:]\n                    tmp[0,4::6,:,:] = tmp[0,:2,:,:]\n                    tmp[0,5::6,:,:] = tmp[0,:2,:,:]\n                    tmp2[0,::6] = tmp2[0,:2]\n                    tmp2[0,1::6] = tmp2[0,:2]\n                    tmp2[0,2::6] = tmp2[0,:2]\n                    tmp2[0,3::6] = tmp2[0,:2]\n                    tmp2[0,4::6] = tmp2[0,:2]\n                    tmp2[0,5::6] = tmp2[0,:2]\n                elif (ct<=3)&(np.random.uniform(0,1)<0.3):\n                    tmp[0,::4,:,:] = tmp[0,:3,:,:]\n                    tmp[0,1::4,:,:] = tmp[0,:3,:,:]\n                    tmp[0,2::4,:,:] = tmp[0,:3,:,:]\n                    tmp[0,3::4,:,:] = tmp[0,:3,:,:]\n                    tmp2[0,::4] = tmp2[0,:3]\n                    tmp2[0,1::4] = tmp2[0,:3]\n                    tmp2[0,2::4] = tmp2[0,:3]\n                    tmp2[0,3::4] = tmp2[0,:3]\n                elif (ct<=4)&(np.random.uniform(0,1)<0.3):\n                    tmp[0,::3,:,:] = tmp[0,:4,:,:]\n                    tmp[0,1::3,:,:] = tmp[0,:4,:,:]\n                    tmp[0,2::3,:,:] = tmp[0,:4,:,:]\n                    tmp2[0,::3] = tmp2[0,:4]\n                    tmp2[0,1::3] = tmp2[0,:4]\n                    tmp2[0,2::3] = tmp2[0,:4]\n                else:\n                    tmp[0,::2,:,:] = tmp[0,:6,:,:]\n                    tmp[0,1::2,:,:] = tmp[0,:6,:,:]\n                    tmp2[0,::2] = tmp2[0,:6]\n                    tmp2[0,1::2] = tmp2[0,:6]\n            \n            X_batch[i*n:(i+1)*n] = tmp\n            non_empty_frame_idxs_batch[i*n:(i+1)*n] = tmp2\n        \n        yield { 'frames': X_batch, 'non_empty_frame_idxs': non_empty_frame_idxs_batch }, y_batch","metadata":{"execution":{"iopub.status.busy":"2023-11-08T10:13:43.578740Z","iopub.execute_input":"2023-11-08T10:13:43.579354Z","iopub.status.idle":"2023-11-08T10:13:43.609629Z","shell.execute_reply.started":"2023-11-08T10:13:43.579324Z","shell.execute_reply":"2023-11-08T10:13:43.608642Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"# MODEL CONFIG","metadata":{}},{"cell_type":"code","source":"# Epsilon value for layer normalisation\nLAYER_NORM_EPS = 1e-6\n\n# Dense layer units for landmarks\nLIPS_UNITS = 224 # WAS 384\nHANDS_UNITS = 224 # WAS 384\nPOSE_UNITS = 224 # WAS 384\n# final embedding and transformer embedding size\nUNITS = 376 # WAS 512\n\n# Transformer\nNUM_BLOCKS = 3 # WAS 2\nMLP_RATIO = 3 # WAS 4\n\n# Dropout\nEMBEDDING_DROPOUT = 0.00\nMLP_DROPOUT_RATIO = 0.30 # WAS 0.40\nCLASSIFIER_DROPOUT_RATIO = 0.10\n\n# Initiailizers\nINIT_HE_UNIFORM = tf.keras.initializers.he_uniform\nINIT_GLOROT_UNIFORM = tf.keras.initializers.glorot_uniform\nINIT_ZEROS = tf.keras.initializers.constant(0.0)\n# Activations\nGELU = tf.keras.activations.gelu\n\nprint(f'UNITS: {UNITS}')","metadata":{"execution":{"iopub.status.busy":"2023-11-08T10:13:56.133558Z","iopub.execute_input":"2023-11-08T10:13:56.134428Z","iopub.status.idle":"2023-11-08T10:13:56.141309Z","shell.execute_reply.started":"2023-11-08T10:13:56.134392Z","shell.execute_reply":"2023-11-08T10:13:56.140372Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"UNITS: 376\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# TRANSFORMER MODEL","metadata":{}},{"cell_type":"code","source":"# based on: https://stackoverflow.com/\n#questions/67342988/verifying-the-implementation-of-multihead-attention-in-transformer\n# replaced softmax with softmax layer to support masked softmax\ndef scaled_dot_product(q,k,v, softmax, attention_mask):\n    #calculates Q . K(transpose)\n    qkt = tf.matmul(q,k,transpose_b=True)\n    #caculates scaling factor\n    dk = tf.math.sqrt(tf.cast(q.shape[-1],dtype=tf.float32))\n    scaled_qkt = qkt/dk\n    softmax = softmax(scaled_qkt, mask=attention_mask)\n    \n    z = tf.matmul(softmax,v)\n    #shape: (m,Tx,depth), same shape as q,k,v\n    return z\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self,d_model,num_of_heads):\n        super(MultiHeadAttention,self).__init__()\n        self.d_model = d_model\n        self.num_of_heads = num_of_heads\n        self.depth = d_model//num_of_heads\n        self.wq = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n        self.wk = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n        self.wv = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n        self.wo = tf.keras.layers.Dense(d_model)\n        self.softmax = tf.keras.layers.Softmax()\n        \n    def call(self,x, attention_mask):\n        \n        multi_attn = []\n        for i in range(self.num_of_heads):\n            Q = self.wq[i](x)\n            K = self.wk[i](x)\n            V = self.wv[i](x)\n            multi_attn.append(scaled_dot_product(Q,K,V, self.softmax, attention_mask))\n            \n        multi_head = tf.concat(multi_attn,axis=-1)\n        multi_head_attention = self.wo(multi_head)\n        return multi_head_attention","metadata":{"execution":{"iopub.status.busy":"2023-11-08T10:14:11.475697Z","iopub.execute_input":"2023-11-08T10:14:11.476537Z","iopub.status.idle":"2023-11-08T10:14:11.487521Z","shell.execute_reply.started":"2023-11-08T10:14:11.476508Z","shell.execute_reply":"2023-11-08T10:14:11.486574Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Full Transformer\nclass Transformer(tf.keras.Model):\n    def __init__(self, num_blocks):\n        super(Transformer, self).__init__(name='transformer')\n        self.num_blocks = num_blocks\n    \n    def build(self, input_shape):\n        self.ln_1s = []\n        self.mhas = []\n        self.ln_2s = []\n        self.mlps = []\n        # Make Transformer Blocks\n        for i in range(self.num_blocks):\n            # Multi Head Attention\n            self.mhas.append(MultiHeadAttention(UNITS, 8))\n            # Multi Layer Perception\n            self.mlps.append(tf.keras.Sequential([\n                tf.keras.layers.Dense(UNITS * MLP_RATIO, activation=GELU, kernel_initializer=INIT_GLOROT_UNIFORM),\n                tf.keras.layers.Dropout(MLP_DROPOUT_RATIO),\n                tf.keras.layers.Dense(UNITS, kernel_initializer=INIT_HE_UNIFORM),\n            ]))\n        \n    def call(self, x, attention_mask):\n        # Iterate input over transformer blocks\n        for mha, mlp in zip(self.mhas, self.mlps):\n            x = x + mha(x, attention_mask)\n            x = x + mlp(x)\n    \n        return x","metadata":{"execution":{"iopub.status.busy":"2023-11-08T10:14:16.055611Z","iopub.execute_input":"2023-11-08T10:14:16.056318Z","iopub.status.idle":"2023-11-08T10:14:16.064765Z","shell.execute_reply.started":"2023-11-08T10:14:16.056285Z","shell.execute_reply":"2023-11-08T10:14:16.063776Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"class LandmarkEmbedding(tf.keras.Model):\n    def __init__(self, units, name):\n        super(LandmarkEmbedding, self).__init__(name=f'{name}_embedding')\n        self.units = units\n        \n    def build(self, input_shape):\n        # Embedding for missing landmark in frame, initizlied with zeros\n        self.empty_embedding = self.add_weight(\n            name=f'{self.name}_empty_embedding',\n            shape=[self.units],\n            initializer=INIT_ZEROS,\n        )\n        # Embedding\n        self.dense = tf.keras.Sequential([\n            tf.keras.layers.Dense(self.units, name=f'{self.name}_dense_1', use_bias=False, kernel_initializer=INIT_GLOROT_UNIFORM),\n            tf.keras.layers.Activation(GELU),\n            tf.keras.layers.Dense(self.units, name=f'{self.name}_dense_2', use_bias=False, kernel_initializer=INIT_HE_UNIFORM),\n        ], name=f'{self.name}_dense')\n\n    def call(self, x):\n        return tf.where(\n                # Checks whether landmark is missing in frame\n                tf.reduce_sum(x, axis=2, keepdims=True) == 0,\n                # If so, the empty embedding is used\n                self.empty_embedding,\n                # Otherwise the landmark data is embedded\n                self.dense(x),\n            )","metadata":{"execution":{"iopub.status.busy":"2023-11-08T10:14:22.407308Z","iopub.execute_input":"2023-11-08T10:14:22.407950Z","iopub.status.idle":"2023-11-08T10:14:22.416337Z","shell.execute_reply.started":"2023-11-08T10:14:22.407919Z","shell.execute_reply":"2023-11-08T10:14:22.415366Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"class Embedding(tf.keras.Model):\n    def __init__(self):\n        super(Embedding, self).__init__()\n        \n    def get_diffs(self, l):\n        S = l.shape[2]\n        other = tf.expand_dims(l, 3)\n        other = tf.repeat(other, S, axis=3)\n        other = tf.transpose(other, [0,1,3,2])\n        diffs = tf.expand_dims(l, 3) - other\n        diffs = tf.reshape(diffs, [-1, INPUT_SIZE, S*S])\n        return diffs\n\n    def build(self, input_shape):\n        # Positional Embedding, initialized with zeros\n        self.positional_embedding = tf.keras.layers.Embedding(INPUT_SIZE+1, UNITS, embeddings_initializer=INIT_ZEROS)\n        # Embedding layer for Landmarks\n        self.lips_embedding = LandmarkEmbedding(LIPS_UNITS, 'lips')\n        self.left_hand_embedding = LandmarkEmbedding(HANDS_UNITS, 'left_hand')\n        self.pose_embedding = LandmarkEmbedding(POSE_UNITS, 'pose')\n        # Landmark Weights\n        self.landmark_weights = tf.Variable(tf.zeros([3], dtype=tf.float32), name='landmark_weights')\n        # Fully Connected Layers for combined landmarks\n        self.fc = tf.keras.Sequential([\n            tf.keras.layers.Dense(UNITS, name='fully_connected_1', use_bias=False, kernel_initializer=INIT_GLOROT_UNIFORM),\n            tf.keras.layers.Activation(GELU),\n            tf.keras.layers.Dense(UNITS, name='fully_connected_2', use_bias=False, kernel_initializer=INIT_HE_UNIFORM),\n        ], name='fc')\n\n\n    def call(self, lips0, left_hand0, pose0, non_empty_frame_idxs, training=False):\n        # Lips\n        lips_embedding = self.lips_embedding(lips0)\n        # Left Hand\n        left_hand_embedding = self.left_hand_embedding(left_hand0)\n        # Pose\n        pose_embedding = self.pose_embedding(pose0)\n        # Merge Embeddings of all landmarks with mean pooling\n        x = tf.stack((\n            lips_embedding, left_hand_embedding, pose_embedding,\n        ), axis=3)\n        x = x * tf.nn.softmax(self.landmark_weights)\n        x = tf.reduce_sum(x, axis=3)\n        # Fully Connected Layers\n        x = self.fc(x)\n        # Add Positional Embedding\n        max_frame_idxs = tf.clip_by_value(\n                tf.reduce_max(non_empty_frame_idxs, axis=1, keepdims=True),\n                1,\n                np.PINF,\n            )\n        normalised_non_empty_frame_idxs = tf.where(\n            tf.math.equal(non_empty_frame_idxs, -1.0),\n            INPUT_SIZE,\n            tf.cast(\n                non_empty_frame_idxs / max_frame_idxs * INPUT_SIZE,\n                tf.int32,\n            ),\n        )\n        x = x + self.positional_embedding(normalised_non_empty_frame_idxs)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2023-11-08T10:14:27.084843Z","iopub.execute_input":"2023-11-08T10:14:27.085214Z","iopub.status.idle":"2023-11-08T10:14:27.099975Z","shell.execute_reply.started":"2023-11-08T10:14:27.085187Z","shell.execute_reply":"2023-11-08T10:14:27.098984Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"# LOSS FUNCTION","metadata":{}},{"cell_type":"code","source":"# source:: https://stackoverflow.com/questions/60689185/label-smoothing-for-sparse-categorical-crossentropy\ndef scce_with_ls(y_true, y_pred):\n    # One Hot Encode Sparsely Encoded Target Sign\n    y_true = tf.cast(y_true, tf.int32)\n    y_true = tf.one_hot(y_true, NUM_CLASSES, axis=1)\n    y_true = tf.squeeze(y_true, axis=2)\n    # Categorical Crossentropy with native label smoothing support\n    return tf.keras.losses.categorical_crossentropy(y_true, y_pred, label_smoothing=0.25)","metadata":{"execution":{"iopub.status.busy":"2023-11-08T10:14:34.216423Z","iopub.execute_input":"2023-11-08T10:14:34.217294Z","iopub.status.idle":"2023-11-08T10:14:34.222573Z","shell.execute_reply.started":"2023-11-08T10:14:34.217262Z","shell.execute_reply":"2023-11-08T10:14:34.221596Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"# BUILD MODEL","metadata":{}},{"cell_type":"code","source":"def get_model():\n    # Inputs\n    frames = tf.keras.layers.Input([INPUT_SIZE, N_COLS, N_DIMS], dtype=tf.float32, name='frames')\n    non_empty_frame_idxs = tf.keras.layers.Input([INPUT_SIZE], dtype=tf.float32, name='non_empty_frame_idxs')\n    # Padding Mask\n    mask0 = tf.cast(tf.math.not_equal(non_empty_frame_idxs, -1), tf.float32)\n    mask = tf.expand_dims(mask0, axis=2)\n    \n    \"\"\"\n        left_hand: 468:489\n        pose: 489:522\n        right_hand: 522:543\n    \"\"\"\n    x = frames\n    x = tf.slice(x, [0,0,0,0], [-1,INPUT_SIZE, N_COLS, 2])\n    # LIPS\n    lips = tf.slice(x, [0,0,LIPS_START,0], [-1,INPUT_SIZE, 40, 2])\n    lips = tf.where(\n            tf.math.equal(lips, 0.0),\n            0.0,\n            (lips - LIPS_MEAN) / LIPS_STD,\n        )\n    # LEFT HAND\n    left_hand = tf.slice(x, [0,0,40,0], [-1,INPUT_SIZE, 21, 2])\n    left_hand = tf.where(\n            tf.math.equal(left_hand, 0.0),\n            0.0,\n            (left_hand - LEFT_HANDS_MEAN) / LEFT_HANDS_STD,\n        )\n    # POSE\n    pose = tf.slice(x, [0,0,61,0], [-1,INPUT_SIZE, 5, 2])\n    pose = tf.where(\n            tf.math.equal(pose, 0.0),\n            0.0,\n            (pose - POSE_MEAN) / POSE_STD,\n        )\n    \n    # Flatten\n    lips = tf.reshape(lips, [-1, INPUT_SIZE, 40*2])\n    left_hand = tf.reshape(left_hand, [-1, INPUT_SIZE, 21*2])\n    pose = tf.reshape(pose, [-1, INPUT_SIZE, 5*2])\n        \n    # Embedding\n    x = Embedding()(lips, left_hand, pose, non_empty_frame_idxs)\n    \n    # Encoder Transformer Blocks\n    x = Transformer(NUM_BLOCKS)(x, mask)\n    \n    # Pooling\n    x = tf.reduce_sum(x * mask, axis=1) / tf.reduce_sum(mask, axis=1)\n    # Classifier Dropout\n    x = tf.keras.layers.Dropout(CLASSIFIER_DROPOUT_RATIO)(x)\n    # Classification Layer\n    x = tf.keras.layers.Dense(NUM_CLASSES, activation=tf.keras.activations.softmax, kernel_initializer=INIT_GLOROT_UNIFORM)(x)\n    \n    outputs = x\n    \n    # Create Tensorflow Model\n    model = tf.keras.models.Model(inputs=[frames, non_empty_frame_idxs], outputs=outputs)\n    \n    # Sparse Categorical Cross Entropy With Label Smoothing\n    loss = scce_with_ls\n    optimizer = tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-5, clipnorm=1.0)\n    # TopK Metrics\n    metrics = [\n        tf.keras.metrics.SparseCategoricalAccuracy(name='acc'),\n        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top_5_acc'),\n        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=10, name='top_10_acc'),\n    ]\n    \n    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-11-08T10:14:43.610009Z","iopub.execute_input":"2023-11-08T10:14:43.610855Z","iopub.status.idle":"2023-11-08T10:14:43.625971Z","shell.execute_reply.started":"2023-11-08T10:14:43.610821Z","shell.execute_reply":"2023-11-08T10:14:43.624984Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"# LEARNING RATE SCHEDULER","metadata":{}},{"cell_type":"code","source":"def lrfn(current_step, num_warmup_steps, lr_max, num_cycles=0.50, num_training_steps=N_EPOCHS):\n    \n    if current_step < num_warmup_steps:\n        if WARMUP_METHOD == 'log':\n            return lr_max * 0.10 ** (num_warmup_steps - current_step)\n        else:\n            return lr_max * 2 ** -(num_warmup_steps - current_step)\n    else:\n        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr_max\n    \nLR_SCHEDULE = [lrfn(step, num_warmup_steps=N_WARMUP_EPOCHS, lr_max=LR_MAX, num_cycles=0.50) for step in range(N_EPOCHS)]\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lambda step: LR_SCHEDULE[step], verbose=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-08T10:14:48.461498Z","iopub.execute_input":"2023-11-08T10:14:48.461912Z","iopub.status.idle":"2023-11-08T10:14:48.470704Z","shell.execute_reply.started":"2023-11-08T10:14:48.461868Z","shell.execute_reply":"2023-11-08T10:14:48.469729Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"# WEIGHT DECAY CALLBACK","metadata":{}},{"cell_type":"code","source":"# Custom callback to update weight decay with learning rate\nclass WeightDecayCallback(tf.keras.callbacks.Callback):\n    def __init__(self, wd_ratio=WD_RATIO):\n        self.step_counter = 0\n        self.wd_ratio = wd_ratio\n    \n    def on_epoch_begin(self, epoch, logs=None):\n        model.optimizer.weight_decay = model.optimizer.learning_rate * self.wd_ratio\n        #print(f'learning rate: {model.optimizer.learning_rate.numpy():.2e}, weight decay: {model.optimizer.weight_decay.numpy():.2e}')","metadata":{"execution":{"iopub.status.busy":"2023-11-08T10:14:51.331833Z","iopub.execute_input":"2023-11-08T10:14:51.332190Z","iopub.status.idle":"2023-11-08T10:14:51.337900Z","shell.execute_reply.started":"2023-11-08T10:14:51.332163Z","shell.execute_reply":"2023-11-08T10:14:51.336951Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"# TRAIN MODEL","metadata":{}},{"cell_type":"code","source":"models = []\n\nprint('#'*25)\nprint('### Training Model',1,'...')\nprint('#'*25)\n\n# # Clear all models in GPU\n# VERBOSE = 0; VERBOSE2 = 0\n# if k==0: \n#     tf.keras.backend.clear_session()\n#     VERBOSE = 2; VERBOSE2 = 1\n\n# Get new fresh model\nmodel = get_model()\n\n# Sanity Check\n#model.summary()\n\n# Actual Training\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lambda step: LR_SCHEDULE[step], \n                                                       verbose=1)\nhistory = model.fit(\n        x=get_train_batch_all_signs(X_train, y_train, NON_EMPTY_FRAME_IDXS_TRAIN),\n        steps_per_epoch=len(X_train) // (NUM_CLASSES * BATCH_ALL_SIGNS_N),\n        epochs=N_EPOCHS,\n        batch_size=BATCH_SIZE,\n        validation_data=validation_data,\n        callbacks=[\n            lr_callback,\n            WeightDecayCallback(),\n        ],\n        verbose = 1,\n    )\n\n# Save Model Weights\nmodel.save_weights('model1.h5')\nmodels.append(model)","metadata":{"execution":{"iopub.status.busy":"2023-11-08T11:10:06.351908Z","iopub.execute_input":"2023-11-08T11:10:06.352281Z","iopub.status.idle":"2023-11-08T11:51:41.442962Z","shell.execute_reply.started":"2023-11-08T11:10:06.352254Z","shell.execute_reply":"2023-11-08T11:51:41.441941Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"#########################\n### Training Model 1 ...\n#########################\n\nEpoch 1: LearningRateScheduler setting learning rate to 0.001.\nEpoch 1/120\n320/320 [==============================] - 41s 73ms/step - loss: 4.1133 - acc: 0.3255 - top_5_acc: 0.6050 - top_10_acc: 0.7057 - val_loss: 3.3856 - val_acc: 0.5218 - val_top_5_acc: 0.8202 - val_top_10_acc: 0.9004 - lr: 0.0010\n\nEpoch 2: LearningRateScheduler setting learning rate to 0.0009998286624877785.\nEpoch 2/120\n320/320 [==============================] - 21s 65ms/step - loss: 3.3164 - acc: 0.5678 - top_5_acc: 0.8209 - top_10_acc: 0.8787 - val_loss: 3.1144 - val_acc: 0.6301 - val_top_5_acc: 0.8731 - val_top_10_acc: 0.9235 - lr: 9.9983e-04\n\nEpoch 3: LearningRateScheduler setting learning rate to 0.0009993147673772868.\nEpoch 3/120\n320/320 [==============================] - 21s 65ms/step - loss: 3.0989 - acc: 0.6458 - top_5_acc: 0.8655 - top_10_acc: 0.9087 - val_loss: 3.0080 - val_acc: 0.6634 - val_top_5_acc: 0.8910 - val_top_10_acc: 0.9348 - lr: 9.9931e-04\n\nEpoch 4: LearningRateScheduler setting learning rate to 0.000998458666866564.\nEpoch 4/120\n320/320 [==============================] - 20s 64ms/step - loss: 2.9834 - acc: 0.6854 - top_5_acc: 0.8870 - top_10_acc: 0.9234 - val_loss: 3.0003 - val_acc: 0.6552 - val_top_5_acc: 0.8937 - val_top_10_acc: 0.9378 - lr: 9.9846e-04\n\nEpoch 5: LearningRateScheduler setting learning rate to 0.0009972609476841367.\nEpoch 5/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.9250 - acc: 0.7078 - top_5_acc: 0.8969 - top_10_acc: 0.9302 - val_loss: 2.9593 - val_acc: 0.6696 - val_top_5_acc: 0.8936 - val_top_10_acc: 0.9368 - lr: 9.9726e-04\n\nEpoch 6: LearningRateScheduler setting learning rate to 0.0009957224306869053.\nEpoch 6/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.8572 - acc: 0.7334 - top_5_acc: 0.9087 - top_10_acc: 0.9385 - val_loss: 2.9039 - val_acc: 0.6878 - val_top_5_acc: 0.9077 - val_top_10_acc: 0.9462 - lr: 9.9572e-04\n\nEpoch 7: LearningRateScheduler setting learning rate to 0.0009938441702975688.\nEpoch 7/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.8153 - acc: 0.7488 - top_5_acc: 0.9163 - top_10_acc: 0.9428 - val_loss: 2.9283 - val_acc: 0.6856 - val_top_5_acc: 0.9024 - val_top_10_acc: 0.9403 - lr: 9.9384e-04\n\nEpoch 8: LearningRateScheduler setting learning rate to 0.0009916274537819774.\nEpoch 8/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.7837 - acc: 0.7612 - top_5_acc: 0.9225 - top_10_acc: 0.9480 - val_loss: 2.9669 - val_acc: 0.6689 - val_top_5_acc: 0.8953 - val_top_10_acc: 0.9372 - lr: 9.9163e-04\n\nEpoch 9: LearningRateScheduler setting learning rate to 0.0009890738003669028.\nEpoch 9/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.7530 - acc: 0.7714 - top_5_acc: 0.9269 - top_10_acc: 0.9503 - val_loss: 2.9185 - val_acc: 0.6850 - val_top_5_acc: 0.9005 - val_top_10_acc: 0.9406 - lr: 9.8907e-04\n\nEpoch 10: LearningRateScheduler setting learning rate to 0.0009861849601988384.\nEpoch 10/120\n320/320 [==============================] - 20s 64ms/step - loss: 2.7159 - acc: 0.7843 - top_5_acc: 0.9323 - top_10_acc: 0.9543 - val_loss: 2.8942 - val_acc: 0.6987 - val_top_5_acc: 0.9029 - val_top_10_acc: 0.9413 - lr: 9.8618e-04\n\nEpoch 11: LearningRateScheduler setting learning rate to 0.0009829629131445341.\nEpoch 11/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.6880 - acc: 0.7960 - top_5_acc: 0.9372 - top_10_acc: 0.9579 - val_loss: 2.8722 - val_acc: 0.7068 - val_top_5_acc: 0.9076 - val_top_10_acc: 0.9433 - lr: 9.8296e-04\n\nEpoch 12: LearningRateScheduler setting learning rate to 0.0009794098674340967.\nEpoch 12/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.6525 - acc: 0.8091 - top_5_acc: 0.9427 - top_10_acc: 0.9618 - val_loss: 2.8600 - val_acc: 0.7079 - val_top_5_acc: 0.9099 - val_top_10_acc: 0.9443 - lr: 9.7941e-04\n\nEpoch 13: LearningRateScheduler setting learning rate to 0.0009755282581475768.\nEpoch 13/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.6280 - acc: 0.8181 - top_5_acc: 0.9456 - top_10_acc: 0.9647 - val_loss: 2.8811 - val_acc: 0.7012 - val_top_5_acc: 0.9055 - val_top_10_acc: 0.9421 - lr: 9.7553e-04\n\nEpoch 14: LearningRateScheduler setting learning rate to 0.0009713207455460893.\nEpoch 14/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.6005 - acc: 0.8291 - top_5_acc: 0.9521 - top_10_acc: 0.9686 - val_loss: 2.8538 - val_acc: 0.7106 - val_top_5_acc: 0.9095 - val_top_10_acc: 0.9472 - lr: 9.7132e-04\n\nEpoch 15: LearningRateScheduler setting learning rate to 0.0009667902132486009.\nEpoch 15/120\n320/320 [==============================] - 20s 64ms/step - loss: 2.5811 - acc: 0.8355 - top_5_acc: 0.9542 - top_10_acc: 0.9699 - val_loss: 2.8553 - val_acc: 0.7108 - val_top_5_acc: 0.9094 - val_top_10_acc: 0.9437 - lr: 9.6679e-04\n\nEpoch 16: LearningRateScheduler setting learning rate to 0.0009619397662556434.\nEpoch 16/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.5700 - acc: 0.8402 - top_5_acc: 0.9547 - top_10_acc: 0.9701 - val_loss: 2.8490 - val_acc: 0.7174 - val_top_5_acc: 0.9110 - val_top_10_acc: 0.9445 - lr: 9.6194e-04\n\nEpoch 17: LearningRateScheduler setting learning rate to 0.0009567727288213005.\nEpoch 17/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.5503 - acc: 0.8449 - top_5_acc: 0.9598 - top_10_acc: 0.9741 - val_loss: 2.8359 - val_acc: 0.7194 - val_top_5_acc: 0.9103 - val_top_10_acc: 0.9429 - lr: 9.5677e-04\n\nEpoch 18: LearningRateScheduler setting learning rate to 0.0009512926421749304.\nEpoch 18/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.5239 - acc: 0.8575 - top_5_acc: 0.9634 - top_10_acc: 0.9771 - val_loss: 2.8373 - val_acc: 0.7233 - val_top_5_acc: 0.9074 - val_top_10_acc: 0.9406 - lr: 9.5129e-04\n\nEpoch 19: LearningRateScheduler setting learning rate to 0.0009455032620941839.\nEpoch 19/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.5146 - acc: 0.8603 - top_5_acc: 0.9644 - top_10_acc: 0.9773 - val_loss: 2.8204 - val_acc: 0.7224 - val_top_5_acc: 0.9115 - val_top_10_acc: 0.9471 - lr: 9.4550e-04\n\nEpoch 20: LearningRateScheduler setting learning rate to 0.0009394085563309827.\nEpoch 20/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.5018 - acc: 0.8656 - top_5_acc: 0.9663 - top_10_acc: 0.9789 - val_loss: 2.8816 - val_acc: 0.7118 - val_top_5_acc: 0.8995 - val_top_10_acc: 0.9369 - lr: 9.3941e-04\n\nEpoch 21: LearningRateScheduler setting learning rate to 0.0009330127018922195.\nEpoch 21/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.4849 - acc: 0.8715 - top_5_acc: 0.9700 - top_10_acc: 0.9814 - val_loss: 2.8636 - val_acc: 0.7092 - val_top_5_acc: 0.9050 - val_top_10_acc: 0.9415 - lr: 9.3301e-04\n\nEpoch 22: LearningRateScheduler setting learning rate to 0.0009263200821770461.\nEpoch 22/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.4585 - acc: 0.8808 - top_5_acc: 0.9724 - top_10_acc: 0.9835 - val_loss: 2.8338 - val_acc: 0.7249 - val_top_5_acc: 0.9063 - val_top_10_acc: 0.9412 - lr: 9.2632e-04\n\nEpoch 23: LearningRateScheduler setting learning rate to 0.0009193352839727121.\nEpoch 23/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.4506 - acc: 0.8840 - top_5_acc: 0.9743 - top_10_acc: 0.9841 - val_loss: 2.8218 - val_acc: 0.7265 - val_top_5_acc: 0.9100 - val_top_10_acc: 0.9419 - lr: 9.1934e-04\n\nEpoch 24: LearningRateScheduler setting learning rate to 0.0009120630943110077.\nEpoch 24/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.4331 - acc: 0.8901 - top_5_acc: 0.9768 - top_10_acc: 0.9857 - val_loss: 2.8425 - val_acc: 0.7194 - val_top_5_acc: 0.9068 - val_top_10_acc: 0.9422 - lr: 9.1206e-04\n\nEpoch 25: LearningRateScheduler setting learning rate to 0.0009045084971874737.\nEpoch 25/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.4220 - acc: 0.8947 - top_5_acc: 0.9785 - top_10_acc: 0.9869 - val_loss: 2.8296 - val_acc: 0.7233 - val_top_5_acc: 0.9073 - val_top_10_acc: 0.9424 - lr: 9.0451e-04\n\nEpoch 26: LearningRateScheduler setting learning rate to 0.0008966766701456176.\nEpoch 26/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.4061 - acc: 0.9000 - top_5_acc: 0.9803 - top_10_acc: 0.9882 - val_loss: 2.8224 - val_acc: 0.7265 - val_top_5_acc: 0.9110 - val_top_10_acc: 0.9420 - lr: 8.9668e-04\n\nEpoch 27: LearningRateScheduler setting learning rate to 0.0008885729807284854.\nEpoch 27/120\n320/320 [==============================] - 20s 64ms/step - loss: 2.3979 - acc: 0.9044 - top_5_acc: 0.9801 - top_10_acc: 0.9880 - val_loss: 2.8442 - val_acc: 0.7225 - val_top_5_acc: 0.9081 - val_top_10_acc: 0.9401 - lr: 8.8857e-04\n\nEpoch 28: LearningRateScheduler setting learning rate to 0.0008802029828000156.\nEpoch 28/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.3894 - acc: 0.9066 - top_5_acc: 0.9820 - top_10_acc: 0.9892 - val_loss: 2.8195 - val_acc: 0.7301 - val_top_5_acc: 0.9100 - val_top_10_acc: 0.9422 - lr: 8.8020e-04\n\nEpoch 29: LearningRateScheduler setting learning rate to 0.0008715724127386971.\nEpoch 29/120\n320/320 [==============================] - 20s 64ms/step - loss: 2.3732 - acc: 0.9117 - top_5_acc: 0.9835 - top_10_acc: 0.9902 - val_loss: 2.8140 - val_acc: 0.7347 - val_top_5_acc: 0.9122 - val_top_10_acc: 0.9424 - lr: 8.7157e-04\n\nEpoch 30: LearningRateScheduler setting learning rate to 0.0008626871855061438.\nEpoch 30/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.3611 - acc: 0.9162 - top_5_acc: 0.9851 - top_10_acc: 0.9914 - val_loss: 2.7861 - val_acc: 0.7409 - val_top_5_acc: 0.9136 - val_top_10_acc: 0.9451 - lr: 8.6269e-04\n\nEpoch 31: LearningRateScheduler setting learning rate to 0.0008535533905932737.\nEpoch 31/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.3522 - acc: 0.9191 - top_5_acc: 0.9863 - top_10_acc: 0.9923 - val_loss: 2.8068 - val_acc: 0.7313 - val_top_5_acc: 0.9145 - val_top_10_acc: 0.9441 - lr: 8.5355e-04\n\nEpoch 32: LearningRateScheduler setting learning rate to 0.000844177287846877.\nEpoch 32/120\n320/320 [==============================] - 20s 64ms/step - loss: 2.3423 - acc: 0.9246 - top_5_acc: 0.9869 - top_10_acc: 0.9928 - val_loss: 2.7956 - val_acc: 0.7394 - val_top_5_acc: 0.9161 - val_top_10_acc: 0.9429 - lr: 8.4418e-04\n\nEpoch 33: LearningRateScheduler setting learning rate to 0.0008345653031794292.\nEpoch 33/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.3418 - acc: 0.9230 - top_5_acc: 0.9878 - top_10_acc: 0.9935 - val_loss: 2.7766 - val_acc: 0.7441 - val_top_5_acc: 0.9184 - val_top_10_acc: 0.9462 - lr: 8.3457e-04\n\nEpoch 34: LearningRateScheduler setting learning rate to 0.0008247240241650918.\nEpoch 34/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.3253 - acc: 0.9292 - top_5_acc: 0.9889 - top_10_acc: 0.9939 - val_loss: 2.8042 - val_acc: 0.7330 - val_top_5_acc: 0.9153 - val_top_10_acc: 0.9443 - lr: 8.2472e-04\n\nEpoch 35: LearningRateScheduler setting learning rate to 0.0008146601955249188.\nEpoch 35/120\n320/320 [==============================] - 20s 64ms/step - loss: 2.3165 - acc: 0.9325 - top_5_acc: 0.9892 - top_10_acc: 0.9941 - val_loss: 2.8092 - val_acc: 0.7359 - val_top_5_acc: 0.9084 - val_top_10_acc: 0.9403 - lr: 8.1466e-04\n\nEpoch 36: LearningRateScheduler setting learning rate to 0.0008043807145043603.\nEpoch 36/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.3203 - acc: 0.9316 - top_5_acc: 0.9895 - top_10_acc: 0.9944 - val_loss: 2.7994 - val_acc: 0.7372 - val_top_5_acc: 0.9160 - val_top_10_acc: 0.9466 - lr: 8.0438e-04\n\nEpoch 37: LearningRateScheduler setting learning rate to 0.0007938926261462366.\nEpoch 37/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.3035 - acc: 0.9362 - top_5_acc: 0.9911 - top_10_acc: 0.9953 - val_loss: 2.7962 - val_acc: 0.7405 - val_top_5_acc: 0.9101 - val_top_10_acc: 0.9398 - lr: 7.9389e-04\n\nEpoch 38: LearningRateScheduler setting learning rate to 0.0007832031184624164.\nEpoch 38/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.2839 - acc: 0.9436 - top_5_acc: 0.9922 - top_10_acc: 0.9959 - val_loss: 2.8015 - val_acc: 0.7337 - val_top_5_acc: 0.9142 - val_top_10_acc: 0.9458 - lr: 7.8320e-04\n\nEpoch 39: LearningRateScheduler setting learning rate to 0.0007723195175075137.\nEpoch 39/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.2744 - acc: 0.9469 - top_5_acc: 0.9925 - top_10_acc: 0.9960 - val_loss: 2.8086 - val_acc: 0.7330 - val_top_5_acc: 0.9150 - val_top_10_acc: 0.9441 - lr: 7.7232e-04\n\nEpoch 40: LearningRateScheduler setting learning rate to 0.0007612492823579744.\nEpoch 40/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.2586 - acc: 0.9521 - top_5_acc: 0.9938 - top_10_acc: 0.9966 - val_loss: 2.7630 - val_acc: 0.7518 - val_top_5_acc: 0.9161 - val_top_10_acc: 0.9451 - lr: 7.6125e-04\n\nEpoch 41: LearningRateScheduler setting learning rate to 0.00075.\nEpoch 41/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.2543 - acc: 0.9527 - top_5_acc: 0.9941 - top_10_acc: 0.9969 - val_loss: 2.7672 - val_acc: 0.7445 - val_top_5_acc: 0.9166 - val_top_10_acc: 0.9469 - lr: 7.5000e-04\n\nEpoch 42: LearningRateScheduler setting learning rate to 0.0007385793801298042.\nEpoch 42/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.2473 - acc: 0.9540 - top_5_acc: 0.9946 - top_10_acc: 0.9972 - val_loss: 2.7701 - val_acc: 0.7455 - val_top_5_acc: 0.9168 - val_top_10_acc: 0.9461 - lr: 7.3858e-04\n\nEpoch 43: LearningRateScheduler setting learning rate to 0.0007269952498697733.\nEpoch 43/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.2466 - acc: 0.9560 - top_5_acc: 0.9947 - top_10_acc: 0.9971 - val_loss: 2.7736 - val_acc: 0.7410 - val_top_5_acc: 0.9203 - val_top_10_acc: 0.9460 - lr: 7.2700e-04\n\nEpoch 44: LearningRateScheduler setting learning rate to 0.0007152555484041476.\nEpoch 44/120\n320/320 [==============================] - 20s 64ms/step - loss: 2.2346 - acc: 0.9603 - top_5_acc: 0.9954 - top_10_acc: 0.9974 - val_loss: 2.7734 - val_acc: 0.7412 - val_top_5_acc: 0.9169 - val_top_10_acc: 0.9450 - lr: 7.1526e-04\n\nEpoch 45: LearningRateScheduler setting learning rate to 0.0007033683215379002.\nEpoch 45/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.2286 - acc: 0.9614 - top_5_acc: 0.9956 - top_10_acc: 0.9976 - val_loss: 2.7746 - val_acc: 0.7433 - val_top_5_acc: 0.9150 - val_top_10_acc: 0.9438 - lr: 7.0337e-04\n\nEpoch 46: LearningRateScheduler setting learning rate to 0.000691341716182545.\nEpoch 46/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.2185 - acc: 0.9653 - top_5_acc: 0.9959 - top_10_acc: 0.9978 - val_loss: 2.7633 - val_acc: 0.7492 - val_top_5_acc: 0.9176 - val_top_10_acc: 0.9446 - lr: 6.9134e-04\n\nEpoch 47: LearningRateScheduler setting learning rate to 0.0006791839747726501.\nEpoch 47/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.2133 - acc: 0.9655 - top_5_acc: 0.9965 - top_10_acc: 0.9980 - val_loss: 2.7859 - val_acc: 0.7417 - val_top_5_acc: 0.9125 - val_top_10_acc: 0.9413 - lr: 6.7918e-04\n\nEpoch 48: LearningRateScheduler setting learning rate to 0.0006669034296168854.\nEpoch 48/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.2128 - acc: 0.9661 - top_5_acc: 0.9962 - top_10_acc: 0.9980 - val_loss: 2.7663 - val_acc: 0.7480 - val_top_5_acc: 0.9201 - val_top_10_acc: 0.9481 - lr: 6.6690e-04\n\nEpoch 49: LearningRateScheduler setting learning rate to 0.0006545084971874737.\nEpoch 49/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.1993 - acc: 0.9702 - top_5_acc: 0.9967 - top_10_acc: 0.9980 - val_loss: 2.7692 - val_acc: 0.7466 - val_top_5_acc: 0.9166 - val_top_10_acc: 0.9448 - lr: 6.5451e-04\n\nEpoch 50: LearningRateScheduler setting learning rate to 0.0006420076723519614.\nEpoch 50/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.1913 - acc: 0.9719 - top_5_acc: 0.9971 - top_10_acc: 0.9983 - val_loss: 2.7759 - val_acc: 0.7402 - val_top_5_acc: 0.9157 - val_top_10_acc: 0.9439 - lr: 6.4201e-04\n\nEpoch 51: LearningRateScheduler setting learning rate to 0.0006294095225512603.\nEpoch 51/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.1873 - acc: 0.9737 - top_5_acc: 0.9970 - top_10_acc: 0.9982 - val_loss: 2.7747 - val_acc: 0.7495 - val_top_5_acc: 0.9147 - val_top_10_acc: 0.9424 - lr: 6.2941e-04\n\nEpoch 52: LearningRateScheduler setting learning rate to 0.0006167226819279528.\nEpoch 52/120\n320/320 [==============================] - 20s 64ms/step - loss: 2.1830 - acc: 0.9743 - top_5_acc: 0.9970 - top_10_acc: 0.9983 - val_loss: 2.7680 - val_acc: 0.7492 - val_top_5_acc: 0.9163 - val_top_10_acc: 0.9438 - lr: 6.1672e-04\n\nEpoch 53: LearningRateScheduler setting learning rate to 0.0006039558454088796.\nEpoch 53/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.1824 - acc: 0.9745 - top_5_acc: 0.9975 - top_10_acc: 0.9985 - val_loss: 2.7514 - val_acc: 0.7567 - val_top_5_acc: 0.9184 - val_top_10_acc: 0.9465 - lr: 6.0396e-04\n\nEpoch 54: LearningRateScheduler setting learning rate to 0.0005911177627460738.\nEpoch 54/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.1730 - acc: 0.9770 - top_5_acc: 0.9977 - top_10_acc: 0.9985 - val_loss: 2.7428 - val_acc: 0.7562 - val_top_5_acc: 0.9206 - val_top_10_acc: 0.9460 - lr: 5.9112e-04\n\nEpoch 55: LearningRateScheduler setting learning rate to 0.0005782172325201155.\nEpoch 55/120\n320/320 [==============================] - 20s 64ms/step - loss: 2.1630 - acc: 0.9790 - top_5_acc: 0.9980 - top_10_acc: 0.9986 - val_loss: 2.7659 - val_acc: 0.7530 - val_top_5_acc: 0.9152 - val_top_10_acc: 0.9420 - lr: 5.7822e-04\n\nEpoch 56: LearningRateScheduler setting learning rate to 0.000565263096110026.\nEpoch 56/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.1599 - acc: 0.9800 - top_5_acc: 0.9977 - top_10_acc: 0.9985 - val_loss: 2.7568 - val_acc: 0.7512 - val_top_5_acc: 0.9164 - val_top_10_acc: 0.9441 - lr: 5.6526e-04\n\nEpoch 57: LearningRateScheduler setting learning rate to 0.0005522642316338268.\nEpoch 57/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.1541 - acc: 0.9819 - top_5_acc: 0.9981 - top_10_acc: 0.9989 - val_loss: 2.7323 - val_acc: 0.7604 - val_top_5_acc: 0.9231 - val_top_10_acc: 0.9490 - lr: 5.5226e-04\n\nEpoch 58: LearningRateScheduler setting learning rate to 0.0005392295478639225.\nEpoch 58/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.1487 - acc: 0.9824 - top_5_acc: 0.9981 - top_10_acc: 0.9987 - val_loss: 2.7580 - val_acc: 0.7494 - val_top_5_acc: 0.9193 - val_top_10_acc: 0.9464 - lr: 5.3923e-04\n\nEpoch 59: LearningRateScheduler setting learning rate to 0.000526167978121472.\nEpoch 59/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.1439 - acc: 0.9836 - top_5_acc: 0.9980 - top_10_acc: 0.9987 - val_loss: 2.7466 - val_acc: 0.7556 - val_top_5_acc: 0.9196 - val_top_10_acc: 0.9464 - lr: 5.2617e-04\n\nEpoch 60: LearningRateScheduler setting learning rate to 0.0005130884741539367.\nEpoch 60/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.1373 - acc: 0.9852 - top_5_acc: 0.9979 - top_10_acc: 0.9987 - val_loss: 2.7395 - val_acc: 0.7600 - val_top_5_acc: 0.9221 - val_top_10_acc: 0.9464 - lr: 5.1309e-04\n\nEpoch 61: LearningRateScheduler setting learning rate to 0.0005.\nEpoch 61/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.1347 - acc: 0.9852 - top_5_acc: 0.9983 - top_10_acc: 0.9990 - val_loss: 2.7537 - val_acc: 0.7545 - val_top_5_acc: 0.9193 - val_top_10_acc: 0.9471 - lr: 5.0000e-04\n\nEpoch 62: LearningRateScheduler setting learning rate to 0.0004869115258460635.\nEpoch 62/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.1276 - acc: 0.9866 - top_5_acc: 0.9986 - top_10_acc: 0.9991 - val_loss: 2.7509 - val_acc: 0.7621 - val_top_5_acc: 0.9201 - val_top_10_acc: 0.9476 - lr: 4.8691e-04\n\nEpoch 63: LearningRateScheduler setting learning rate to 0.0004738320218785281.\nEpoch 63/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.1236 - acc: 0.9871 - top_5_acc: 0.9985 - top_10_acc: 0.9990 - val_loss: 2.7412 - val_acc: 0.7614 - val_top_5_acc: 0.9203 - val_top_10_acc: 0.9457 - lr: 4.7383e-04\n\nEpoch 64: LearningRateScheduler setting learning rate to 0.0004607704521360776.\nEpoch 64/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.1200 - acc: 0.9885 - top_5_acc: 0.9984 - top_10_acc: 0.9990 - val_loss: 2.7561 - val_acc: 0.7574 - val_top_5_acc: 0.9201 - val_top_10_acc: 0.9453 - lr: 4.6077e-04\n\nEpoch 65: LearningRateScheduler setting learning rate to 0.00044773576836617336.\nEpoch 65/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.1166 - acc: 0.9883 - top_5_acc: 0.9987 - top_10_acc: 0.9991 - val_loss: 2.7496 - val_acc: 0.7594 - val_top_5_acc: 0.9198 - val_top_10_acc: 0.9457 - lr: 4.4774e-04\n\nEpoch 66: LearningRateScheduler setting learning rate to 0.00043473690388997434.\nEpoch 66/120\n320/320 [==============================] - 20s 64ms/step - loss: 2.1092 - acc: 0.9902 - top_5_acc: 0.9985 - top_10_acc: 0.9990 - val_loss: 2.7405 - val_acc: 0.7629 - val_top_5_acc: 0.9215 - val_top_10_acc: 0.9460 - lr: 4.3474e-04\n\nEpoch 67: LearningRateScheduler setting learning rate to 0.0004217827674798845.\nEpoch 67/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.1053 - acc: 0.9904 - top_5_acc: 0.9986 - top_10_acc: 0.9990 - val_loss: 2.7565 - val_acc: 0.7568 - val_top_5_acc: 0.9188 - val_top_10_acc: 0.9453 - lr: 4.2178e-04\n\nEpoch 68: LearningRateScheduler setting learning rate to 0.00040888223725392626.\nEpoch 68/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.1034 - acc: 0.9906 - top_5_acc: 0.9985 - top_10_acc: 0.9990 - val_loss: 2.7378 - val_acc: 0.7605 - val_top_5_acc: 0.9202 - val_top_10_acc: 0.9467 - lr: 4.0888e-04\n\nEpoch 69: LearningRateScheduler setting learning rate to 0.0003960441545911204.\nEpoch 69/120\n320/320 [==============================] - 20s 64ms/step - loss: 2.0972 - acc: 0.9921 - top_5_acc: 0.9987 - top_10_acc: 0.9991 - val_loss: 2.7437 - val_acc: 0.7608 - val_top_5_acc: 0.9208 - val_top_10_acc: 0.9470 - lr: 3.9604e-04\n\nEpoch 70: LearningRateScheduler setting learning rate to 0.00038327731807204744.\nEpoch 70/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.0945 - acc: 0.9925 - top_5_acc: 0.9985 - top_10_acc: 0.9990 - val_loss: 2.7512 - val_acc: 0.7621 - val_top_5_acc: 0.9217 - val_top_10_acc: 0.9476 - lr: 3.8328e-04\n\nEpoch 71: LearningRateScheduler setting learning rate to 0.0003705904774487396.\nEpoch 71/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.0912 - acc: 0.9925 - top_5_acc: 0.9988 - top_10_acc: 0.9993 - val_loss: 2.7324 - val_acc: 0.7647 - val_top_5_acc: 0.9179 - val_top_10_acc: 0.9443 - lr: 3.7059e-04\n\nEpoch 72: LearningRateScheduler setting learning rate to 0.0003579923276480387.\nEpoch 72/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.0891 - acc: 0.9920 - top_5_acc: 0.9985 - top_10_acc: 0.9990 - val_loss: 2.7219 - val_acc: 0.7673 - val_top_5_acc: 0.9242 - val_top_10_acc: 0.9488 - lr: 3.5799e-04\n\nEpoch 73: LearningRateScheduler setting learning rate to 0.00034549150281252633.\nEpoch 73/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.0834 - acc: 0.9931 - top_5_acc: 0.9988 - top_10_acc: 0.9991 - val_loss: 2.7362 - val_acc: 0.7664 - val_top_5_acc: 0.9222 - val_top_10_acc: 0.9473 - lr: 3.4549e-04\n\nEpoch 74: LearningRateScheduler setting learning rate to 0.00033309657038311456.\nEpoch 74/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.0811 - acc: 0.9935 - top_5_acc: 0.9989 - top_10_acc: 0.9993 - val_loss: 2.7405 - val_acc: 0.7645 - val_top_5_acc: 0.9206 - val_top_10_acc: 0.9464 - lr: 3.3310e-04\n\nEpoch 75: LearningRateScheduler setting learning rate to 0.00032081602522734986.\nEpoch 75/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.0762 - acc: 0.9940 - top_5_acc: 0.9988 - top_10_acc: 0.9992 - val_loss: 2.7297 - val_acc: 0.7697 - val_top_5_acc: 0.9226 - val_top_10_acc: 0.9476 - lr: 3.2082e-04\n\nEpoch 76: LearningRateScheduler setting learning rate to 0.0003086582838174551.\nEpoch 76/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.0725 - acc: 0.9948 - top_5_acc: 0.9990 - top_10_acc: 0.9992 - val_loss: 2.7454 - val_acc: 0.7636 - val_top_5_acc: 0.9203 - val_top_10_acc: 0.9465 - lr: 3.0866e-04\n\nEpoch 77: LearningRateScheduler setting learning rate to 0.0002966316784621.\nEpoch 77/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.0702 - acc: 0.9947 - top_5_acc: 0.9987 - top_10_acc: 0.9991 - val_loss: 2.7161 - val_acc: 0.7724 - val_top_5_acc: 0.9245 - val_top_10_acc: 0.9502 - lr: 2.9663e-04\n\nEpoch 78: LearningRateScheduler setting learning rate to 0.0002847444515958523.\nEpoch 78/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.0671 - acc: 0.9947 - top_5_acc: 0.9988 - top_10_acc: 0.9991 - val_loss: 2.7501 - val_acc: 0.7638 - val_top_5_acc: 0.9205 - val_top_10_acc: 0.9457 - lr: 2.8474e-04\n\nEpoch 79: LearningRateScheduler setting learning rate to 0.00027300475013022663.\nEpoch 79/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.0648 - acc: 0.9950 - top_5_acc: 0.9988 - top_10_acc: 0.9990 - val_loss: 2.7282 - val_acc: 0.7675 - val_top_5_acc: 0.9241 - val_top_10_acc: 0.9493 - lr: 2.7300e-04\n\nEpoch 80: LearningRateScheduler setting learning rate to 0.00026142061987019576.\nEpoch 80/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.0625 - acc: 0.9950 - top_5_acc: 0.9988 - top_10_acc: 0.9991 - val_loss: 2.7354 - val_acc: 0.7673 - val_top_5_acc: 0.9220 - val_top_10_acc: 0.9482 - lr: 2.6142e-04\n\nEpoch 81: LearningRateScheduler setting learning rate to 0.0002500000000000001.\nEpoch 81/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.0586 - acc: 0.9959 - top_5_acc: 0.9988 - top_10_acc: 0.9991 - val_loss: 2.7430 - val_acc: 0.7645 - val_top_5_acc: 0.9217 - val_top_10_acc: 0.9470 - lr: 2.5000e-04\n\nEpoch 82: LearningRateScheduler setting learning rate to 0.00023875071764202561.\nEpoch 82/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.0553 - acc: 0.9959 - top_5_acc: 0.9990 - top_10_acc: 0.9992 - val_loss: 2.7509 - val_acc: 0.7642 - val_top_5_acc: 0.9194 - val_top_10_acc: 0.9462 - lr: 2.3875e-04\n\nEpoch 83: LearningRateScheduler setting learning rate to 0.00022768048249248646.\nEpoch 83/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.0520 - acc: 0.9958 - top_5_acc: 0.9990 - top_10_acc: 0.9992 - val_loss: 2.7266 - val_acc: 0.7692 - val_top_5_acc: 0.9238 - val_top_10_acc: 0.9482 - lr: 2.2768e-04\n\nEpoch 84: LearningRateScheduler setting learning rate to 0.0002167968815375837.\nEpoch 84/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.0514 - acc: 0.9959 - top_5_acc: 0.9987 - top_10_acc: 0.9991 - val_loss: 2.7303 - val_acc: 0.7704 - val_top_5_acc: 0.9238 - val_top_10_acc: 0.9484 - lr: 2.1680e-04\n\nEpoch 85: LearningRateScheduler setting learning rate to 0.00020610737385376348.\nEpoch 85/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.0468 - acc: 0.9966 - top_5_acc: 0.9990 - top_10_acc: 0.9994 - val_loss: 2.7226 - val_acc: 0.7733 - val_top_5_acc: 0.9237 - val_top_10_acc: 0.9506 - lr: 2.0611e-04\n\nEpoch 86: LearningRateScheduler setting learning rate to 0.00019561928549563967.\nEpoch 86/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.0454 - acc: 0.9962 - top_5_acc: 0.9991 - top_10_acc: 0.9994 - val_loss: 2.7382 - val_acc: 0.7706 - val_top_5_acc: 0.9239 - val_top_10_acc: 0.9497 - lr: 1.9562e-04\n\nEpoch 87: LearningRateScheduler setting learning rate to 0.00018533980447508135.\nEpoch 87/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.0428 - acc: 0.9969 - top_5_acc: 0.9992 - top_10_acc: 0.9994 - val_loss: 2.7403 - val_acc: 0.7714 - val_top_5_acc: 0.9220 - val_top_10_acc: 0.9487 - lr: 1.8534e-04\n\nEpoch 88: LearningRateScheduler setting learning rate to 0.00017527597583490823.\nEpoch 88/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.0420 - acc: 0.9966 - top_5_acc: 0.9990 - top_10_acc: 0.9993 - val_loss: 2.7351 - val_acc: 0.7716 - val_top_5_acc: 0.9248 - val_top_10_acc: 0.9486 - lr: 1.7528e-04\n\nEpoch 89: LearningRateScheduler setting learning rate to 0.00016543469682057105.\nEpoch 89/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.0400 - acc: 0.9967 - top_5_acc: 0.9990 - top_10_acc: 0.9994 - val_loss: 2.7377 - val_acc: 0.7677 - val_top_5_acc: 0.9271 - val_top_10_acc: 0.9505 - lr: 1.6543e-04\n\nEpoch 90: LearningRateScheduler setting learning rate to 0.00015582271215312294.\nEpoch 90/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.0364 - acc: 0.9970 - top_5_acc: 0.9992 - top_10_acc: 0.9994 - val_loss: 2.7285 - val_acc: 0.7708 - val_top_5_acc: 0.9243 - val_top_10_acc: 0.9486 - lr: 1.5582e-04\n\nEpoch 91: LearningRateScheduler setting learning rate to 0.00014644660940672628.\nEpoch 91/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.0348 - acc: 0.9970 - top_5_acc: 0.9991 - top_10_acc: 0.9994 - val_loss: 2.7463 - val_acc: 0.7679 - val_top_5_acc: 0.9231 - val_top_10_acc: 0.9483 - lr: 1.4645e-04\n\nEpoch 92: LearningRateScheduler setting learning rate to 0.0001373128144938563.\nEpoch 92/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.0333 - acc: 0.9973 - top_5_acc: 0.9991 - top_10_acc: 0.9994 - val_loss: 2.7266 - val_acc: 0.7741 - val_top_5_acc: 0.9243 - val_top_10_acc: 0.9485 - lr: 1.3731e-04\n\nEpoch 93: LearningRateScheduler setting learning rate to 0.00012842758726130281.\nEpoch 93/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.0315 - acc: 0.9971 - top_5_acc: 0.9992 - top_10_acc: 0.9993 - val_loss: 2.7309 - val_acc: 0.7742 - val_top_5_acc: 0.9248 - val_top_10_acc: 0.9486 - lr: 1.2843e-04\n\nEpoch 94: LearningRateScheduler setting learning rate to 0.00011979701719998454.\nEpoch 94/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.0301 - acc: 0.9972 - top_5_acc: 0.9991 - top_10_acc: 0.9993 - val_loss: 2.7301 - val_acc: 0.7733 - val_top_5_acc: 0.9253 - val_top_10_acc: 0.9488 - lr: 1.1980e-04\n\nEpoch 95: LearningRateScheduler setting learning rate to 0.00011142701927151455.\nEpoch 95/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.0289 - acc: 0.9974 - top_5_acc: 0.9991 - top_10_acc: 0.9993 - val_loss: 2.7352 - val_acc: 0.7727 - val_top_5_acc: 0.9247 - val_top_10_acc: 0.9484 - lr: 1.1143e-04\n\nEpoch 96: LearningRateScheduler setting learning rate to 0.00010332332985438247.\nEpoch 96/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.0275 - acc: 0.9974 - top_5_acc: 0.9990 - top_10_acc: 0.9993 - val_loss: 2.7316 - val_acc: 0.7734 - val_top_5_acc: 0.9259 - val_top_10_acc: 0.9502 - lr: 1.0332e-04\n\nEpoch 97: LearningRateScheduler setting learning rate to 9.549150281252633e-05.\nEpoch 97/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.0263 - acc: 0.9973 - top_5_acc: 0.9991 - top_10_acc: 0.9994 - val_loss: 2.7347 - val_acc: 0.7745 - val_top_5_acc: 0.9244 - val_top_10_acc: 0.9490 - lr: 9.5492e-05\n\nEpoch 98: LearningRateScheduler setting learning rate to 8.793690568899215e-05.\nEpoch 98/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.0241 - acc: 0.9974 - top_5_acc: 0.9991 - top_10_acc: 0.9994 - val_loss: 2.7369 - val_acc: 0.7754 - val_top_5_acc: 0.9237 - val_top_10_acc: 0.9480 - lr: 8.7937e-05\n\nEpoch 99: LearningRateScheduler setting learning rate to 8.066471602728804e-05.\nEpoch 99/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.0237 - acc: 0.9973 - top_5_acc: 0.9992 - top_10_acc: 0.9993 - val_loss: 2.7337 - val_acc: 0.7748 - val_top_5_acc: 0.9260 - val_top_10_acc: 0.9490 - lr: 8.0665e-05\n\nEpoch 100: LearningRateScheduler setting learning rate to 7.367991782295391e-05.\nEpoch 100/120\n320/320 [==============================] - 20s 64ms/step - loss: 2.0211 - acc: 0.9975 - top_5_acc: 0.9991 - top_10_acc: 0.9993 - val_loss: 2.7322 - val_acc: 0.7761 - val_top_5_acc: 0.9259 - val_top_10_acc: 0.9499 - lr: 6.6987e-05\n\nEpoch 102: LearningRateScheduler setting learning rate to 6.059144366901737e-05.\nEpoch 102/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.0198 - acc: 0.9976 - top_5_acc: 0.9993 - top_10_acc: 0.9995 - val_loss: 2.7350 - val_acc: 0.7748 - val_top_5_acc: 0.9243 - val_top_10_acc: 0.9482 - lr: 6.0591e-05\n\nEpoch 103: LearningRateScheduler setting learning rate to 5.449673790581611e-05.\nEpoch 103/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.0184 - acc: 0.9980 - top_5_acc: 0.9992 - top_10_acc: 0.9994 - val_loss: 2.7347 - val_acc: 0.7756 - val_top_5_acc: 0.9253 - val_top_10_acc: 0.9490 - lr: 5.4497e-05\n\nEpoch 104: LearningRateScheduler setting learning rate to 4.87073578250698e-05.\nEpoch 104/120\n320/320 [==============================] - 20s 64ms/step - loss: 2.0179 - acc: 0.9976 - top_5_acc: 0.9990 - top_10_acc: 0.9994 - val_loss: 2.7318 - val_acc: 0.7767 - val_top_5_acc: 0.9263 - val_top_10_acc: 0.9494 - lr: 4.8707e-05\n\nEpoch 105: LearningRateScheduler setting learning rate to 4.322727117869951e-05.\nEpoch 105/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.0169 - acc: 0.9977 - top_5_acc: 0.9992 - top_10_acc: 0.9994 - val_loss: 2.7367 - val_acc: 0.7768 - val_top_5_acc: 0.9250 - val_top_10_acc: 0.9479 - lr: 4.3227e-05\n\nEpoch 106: LearningRateScheduler setting learning rate to 3.806023374435663e-05.\nEpoch 106/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.0159 - acc: 0.9979 - top_5_acc: 0.9994 - top_10_acc: 0.9996 - val_loss: 2.7358 - val_acc: 0.7745 - val_top_5_acc: 0.9248 - val_top_10_acc: 0.9495 - lr: 3.8060e-05\n\nEpoch 107: LearningRateScheduler setting learning rate to 3.3209786751399184e-05.\nEpoch 107/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.0156 - acc: 0.9976 - top_5_acc: 0.9992 - top_10_acc: 0.9994 - val_loss: 2.7315 - val_acc: 0.7768 - val_top_5_acc: 0.9255 - val_top_10_acc: 0.9491 - lr: 3.3210e-05\n\nEpoch 108: LearningRateScheduler setting learning rate to 2.8679254453910786e-05.\nEpoch 108/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.0146 - acc: 0.9977 - top_5_acc: 0.9992 - top_10_acc: 0.9994 - val_loss: 2.7353 - val_acc: 0.7762 - val_top_5_acc: 0.9252 - val_top_10_acc: 0.9495 - lr: 2.8679e-05\n\nEpoch 109: LearningRateScheduler setting learning rate to 2.4471741852423235e-05.\nEpoch 109/120\n320/320 [==============================] - 20s 64ms/step - loss: 2.0135 - acc: 0.9980 - top_5_acc: 0.9994 - top_10_acc: 0.9996 - val_loss: 2.7334 - val_acc: 0.7767 - val_top_5_acc: 0.9253 - val_top_10_acc: 0.9483 - lr: 2.4472e-05\n\nEpoch 110: LearningRateScheduler setting learning rate to 2.0590132565903473e-05.\nEpoch 110/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.0139 - acc: 0.9977 - top_5_acc: 0.9992 - top_10_acc: 0.9995 - val_loss: 2.7338 - val_acc: 0.7765 - val_top_5_acc: 0.9264 - val_top_10_acc: 0.9489 - lr: 2.0590e-05\n\nEpoch 111: LearningRateScheduler setting learning rate to 1.70370868554659e-05.\nEpoch 111/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.0136 - acc: 0.9975 - top_5_acc: 0.9992 - top_10_acc: 0.9994 - val_loss: 2.7305 - val_acc: 0.7774 - val_top_5_acc: 0.9267 - val_top_10_acc: 0.9496 - lr: 1.7037e-05\n\nEpoch 112: LearningRateScheduler setting learning rate to 1.3815039801161721e-05.\nEpoch 112/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.0139 - acc: 0.9976 - top_5_acc: 0.9991 - top_10_acc: 0.9993 - val_loss: 2.7329 - val_acc: 0.7766 - val_top_5_acc: 0.9262 - val_top_10_acc: 0.9498 - lr: 1.3815e-05\n\nEpoch 113: LearningRateScheduler setting learning rate to 1.0926199633097156e-05.\nEpoch 113/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.0127 - acc: 0.9978 - top_5_acc: 0.9992 - top_10_acc: 0.9995 - val_loss: 2.7373 - val_acc: 0.7751 - val_top_5_acc: 0.9257 - val_top_10_acc: 0.9490 - lr: 1.0926e-05\n\nEpoch 114: LearningRateScheduler setting learning rate to 8.372546218022748e-06.\nEpoch 114/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.0120 - acc: 0.9979 - top_5_acc: 0.9993 - top_10_acc: 0.9996 - val_loss: 2.7319 - val_acc: 0.7774 - val_top_5_acc: 0.9266 - val_top_10_acc: 0.9495 - lr: 8.3725e-06\n\nEpoch 115: LearningRateScheduler setting learning rate to 6.15582970243117e-06.\nEpoch 115/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.0122 - acc: 0.9979 - top_5_acc: 0.9991 - top_10_acc: 0.9994 - val_loss: 2.7328 - val_acc: 0.7766 - val_top_5_acc: 0.9262 - val_top_10_acc: 0.9494 - lr: 6.1558e-06\n\nEpoch 116: LearningRateScheduler setting learning rate to 4.277569313094809e-06.\nEpoch 116/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.0112 - acc: 0.9982 - top_5_acc: 0.9995 - top_10_acc: 0.9996 - val_loss: 2.7331 - val_acc: 0.7772 - val_top_5_acc: 0.9267 - val_top_10_acc: 0.9493 - lr: 4.2776e-06\n\nEpoch 117: LearningRateScheduler setting learning rate to 2.739052315863355e-06.\nEpoch 117/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.0114 - acc: 0.9979 - top_5_acc: 0.9994 - top_10_acc: 0.9995 - val_loss: 2.7335 - val_acc: 0.7767 - val_top_5_acc: 0.9267 - val_top_10_acc: 0.9497 - lr: 2.7391e-06\n\nEpoch 118: LearningRateScheduler setting learning rate to 1.541333133436018e-06.\nEpoch 118/120\n320/320 [==============================] - 21s 64ms/step - loss: 2.0105 - acc: 0.9982 - top_5_acc: 0.9994 - top_10_acc: 0.9996 - val_loss: 2.7338 - val_acc: 0.7767 - val_top_5_acc: 0.9264 - val_top_10_acc: 0.9497 - lr: 1.5413e-06\n\nEpoch 119: LearningRateScheduler setting learning rate to 6.852326227130834e-07.\nEpoch 119/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.0112 - acc: 0.9976 - top_5_acc: 0.9993 - top_10_acc: 0.9995 - val_loss: 2.7338 - val_acc: 0.7772 - val_top_5_acc: 0.9264 - val_top_10_acc: 0.9497 - lr: 6.8523e-07\n\nEpoch 120: LearningRateScheduler setting learning rate to 1.7133751222137007e-07.\nEpoch 120/120\n320/320 [==============================] - 21s 65ms/step - loss: 2.0116 - acc: 0.9979 - top_5_acc: 0.9992 - top_10_acc: 0.9994 - val_loss: 2.7337 - val_acc: 0.7772 - val_top_5_acc: 0.9264 - val_top_10_acc: 0.9497 - lr: 1.7134e-07\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# VALIDATION","metadata":{}},{"cell_type":"code","source":"preds = []\nfor k in range(len(models)):\n    # Validation Predictions\n    y_val_pred = models[k].predict({ 'frames': X_val, 'non_empty_frame_idxs': NON_EMPTY_FRAME_IDXS_VAL }, verbose=2)\n    preds.append(y_val_pred)\ny_val_pred = np.mean(preds,axis=0).argmax(axis=1)\nacc = (y_val_pred == y_val).mean()\nprint('Holdout Validation Ensemble ACC =',acc)","metadata":{"execution":{"iopub.status.busy":"2023-11-08T11:52:19.524203Z","iopub.execute_input":"2023-11-08T11:52:19.525130Z","iopub.status.idle":"2023-11-08T11:52:27.264211Z","shell.execute_reply.started":"2023-11-08T11:52:19.525097Z","shell.execute_reply":"2023-11-08T11:52:27.263254Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"446/446 - 7s - 7s/epoch - 16ms/step\nHoldout Validation Ensemble ACC = 0.7772318921953958\n","output_type":"stream"}]}]}