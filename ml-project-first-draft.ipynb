{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Numerical Operations\nimport numpy as np\nimport math\n\n# Reading/Writing Data\nimport pandas as pd\nimport pyarrow.parquet as pq\nimport os\nimport sys\nimport json\n\n# Plotting \nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\n# For Progress Bar\nfrom tqdm.notebook import tqdm\n\n# Processing data\nfrom multiprocessing import Pool\n\n# set the maximum number of rows to display to 500\npd.set_option('display.max_rows', 500)\n\nprint(f'Python V{sys.version}')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-02T09:10:04.840572Z","iopub.execute_input":"2023-11-02T09:10:04.840909Z","iopub.status.idle":"2023-11-02T09:10:06.063016Z","shell.execute_reply.started":"2023-11-02T09:10:04.840874Z","shell.execute_reply":"2023-11-02T09:10:06.062080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEV = True\nDEV = False\nSEED = 42","metadata":{"execution":{"iopub.status.busy":"2023-11-02T09:10:07.849078Z","iopub.execute_input":"2023-11-02T09:10:07.850110Z","iopub.status.idle":"2023-11-02T09:10:07.854326Z","shell.execute_reply.started":"2023-11-02T09:10:07.850075Z","shell.execute_reply":"2023-11-02T09:10:07.853216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Input\n\nnumber of samples in **train.csv**: 94477\n\n- **Each Parquet file has a variable length.. (#frames)**\n- All the frames are of equal size (size: 543)\n    - face: 468\n    - left_hand: 21\n    - pose: 33\n    - right_hand: 21","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/asl-signs/train.csv\")\nprint(f\"\\n... train.csv shape: {train_df.shape}\\n\")\ndisplay(train_df.head())","metadata":{"execution":{"iopub.status.busy":"2023-11-02T09:10:09.217275Z","iopub.execute_input":"2023-11-02T09:10:09.218104Z","iopub.status.idle":"2023-11-02T09:10:09.456076Z","shell.execute_reply.started":"2023-11-02T09:10:09.218073Z","shell.execute_reply":"2023-11-02T09:10:09.455053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_full_path(path):\n    return os.path.join(\"/kaggle/input/asl-signs/\", path)\n\nprint(f\"\\n... parquet path: {get_full_path(train_df['path'].iloc[0])}\\n\")\ntmp_df = pd.read_parquet(get_full_path(train_df['path'].iloc[0]))\nprint(f\"\\n... example parquet shape: {tmp_df.shape}\\n\")\ndisplay(tmp_df)\n\nprint(f\"\\n...unique type: {tmp_df['type'].unique()}\\n\")\n\ndisplay(tmp_df.query(\"type == 'face'\").describe())\ndisplay(tmp_df.query(\"type == 'left_hand'\").describe())\ndisplay(tmp_df.query(\"type == 'right_hand'\").describe())\ndisplay(tmp_df.query(\"type == 'pose'\").describe())\n\nprint(f\"\\n...show landmark_index of frame:\\n\")\nprint(tmp_df.query(\"frame == 20\")['landmark_index'].values)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T09:10:09.775402Z","iopub.execute_input":"2023-11-02T09:10:09.775722Z","iopub.status.idle":"2023-11-02T09:10:10.031036Z","shell.execute_reply.started":"2023-11-02T09:10:09.775697Z","shell.execute_reply":"2023-11-02T09:10:10.030081Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Statistics of data in train","metadata":{}},{"cell_type":"code","source":"print(f\"\\n... without duplicate path: {train_df['path'].nunique() == train_df.shape[0]}\")\n\nfig = px.histogram(train_df, y=\"sign\", color=\"sign\", orientation=\"h\", height=5000,\n    labels={\"count\":\"<b>Total Row Count</b>\"}, title=\"<b>Row Counts by Sign (label)</b>\",\n    category_orders={\"sign\": train_df[\"sign\"].value_counts().index}\n)\nfig.update_yaxes(title_text=\"<b>Total Row Count</b>\")\nfig.update_layout(showlegend=False)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T09:10:11.364783Z","iopub.execute_input":"2023-11-02T09:10:11.365157Z","iopub.status.idle":"2023-11-02T09:10:14.149822Z","shell.execute_reply.started":"2023-11-02T09:10:11.365125Z","shell.execute_reply":"2023-11-02T09:10:14.148857Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Statistics of frames\n\nFrame length\n- Min: 2\n- Max: 537\n\n```\ncount    94477.000000\nmean        37.935021\nstd         44.177069\nmin          2.000000\n1%           6.000000\n5%           6.000000\n25%         12.000000\n50%         22.000000\n75%         44.000000\n95%        135.000000\n99%        219.000000\n99.9%      300.524000\nmax        537.000000\n```\n\nMissing Frame\n- Min: 0\n- Min with missing: 1\n- Max: 102\n- Has Missing Frame: 43","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"if DEV:\n    train_df = pd.read_csv('/kaggle/input/asl-signs/train.csv').sample(int(5e3), random_state=SEED)\nelse:\n    train_df = pd.read_csv('/kaggle/input/asl-signs/train.csv')\n    \nwith open(\"/kaggle/input/asl-signs/sign_to_prediction_index_map.json\", \"r\") as f:\n    dict = json.load(f)    \n\nN_SAMPLES = len(train_df)\nprint(f'N_SAMPLES: {N_SAMPLES}')","metadata":{"execution":{"iopub.status.busy":"2023-11-02T09:10:14.151660Z","iopub.execute_input":"2023-11-02T09:10:14.152163Z","iopub.status.idle":"2023-11-02T09:10:14.294937Z","shell.execute_reply.started":"2023-11-02T09:10:14.152119Z","shell.execute_reply":"2023-11-02T09:10:14.294011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_unique_missing(path):\n    df = pd.read_parquet(get_full_path(path))\n    n_unique_frames = df['frame'].nunique()\n    n_miss_frames = df['frame'].max() - df['frame'].min() + 1 - n_unique_frames\n    return n_unique_frames, n_miss_frames\n\nwith Pool() as pool:\n    results = list(tqdm(pool.imap(get_unique_missing, train_df['path']), total=len(train_df)))\n    \nN_UNIQUE_FRAMES = np.array([result[0] for result in results])\nN_MISS_FRAMES = np.array([result[1] for result in results])","metadata":{"execution":{"iopub.status.busy":"2023-11-02T09:10:14.296928Z","iopub.execute_input":"2023-11-02T09:10:14.297310Z","iopub.status.idle":"2023-11-02T09:23:39.386227Z","shell.execute_reply.started":"2023-11-02T09:10:14.297275Z","shell.execute_reply":"2023-11-02T09:23:39.385067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Total Frame Count\nfig = px.histogram(N_UNIQUE_FRAMES, title=f\"<b>Total Frame Count (#samples={N_SAMPLES})</b>\")\nfig.update_xaxes(title_text=\"<b>Frame Count</b>\")\nfig.update_yaxes(title_text=\"<b>Frequency</b>\")\nfig.update_layout(showlegend=False)\nfig.show()\n\nprint(f\"... Min: {N_UNIQUE_FRAMES.min()}\\n\")\nprint(f\"... Max: {N_UNIQUE_FRAMES.max()}\\n\")\n\nPERCENTILES = [0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99, 0.999]\ndisplay(pd.Series(N_UNIQUE_FRAMES).describe(percentiles=PERCENTILES))","metadata":{"execution":{"iopub.status.busy":"2023-11-02T09:23:39.388423Z","iopub.execute_input":"2023-11-02T09:23:39.389476Z","iopub.status.idle":"2023-11-02T09:23:39.629695Z","shell.execute_reply.started":"2023-11-02T09:23:39.389439Z","shell.execute_reply":"2023-11-02T09:23:39.628791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing Frame Count\n# fig = px.histogram(N_MISS_FRAMES, title=f\"<b>Missing Frame Count (#samples={N_SAMPLES})</b>\")\n# fig.update_xaxes(title_text=\"<b>Frame Count</b>\")\n# fig.update_yaxes(title_text=\"<b>Frequency</b>\")\n# fig.update_layout(showlegend=False)\n# fig.show()\n\nprint(f\"... Min(with miss): {N_MISS_FRAMES[N_MISS_FRAMES > 0].min()}\\n\")\nprint(f\"... Max: {N_MISS_FRAMES.max()}\\n\")\nprint(f\"... Has Missing Frame Count: {(N_MISS_FRAMES > 0).sum()}\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-11-02T09:23:39.630964Z","iopub.execute_input":"2023-11-02T09:23:39.631335Z","iopub.status.idle":"2023-11-02T09:23:39.637940Z","shell.execute_reply.started":"2023-11-02T09:23:39.631302Z","shell.execute_reply":"2023-11-02T09:23:39.637099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### (ToDo) Inspect >300 & <6","metadata":{}},{"cell_type":"code","source":"idx_gt_300 = np.where(N_UNIQUE_FRAMES > 300)\nprint(f\"\\n... #frames greater than 300: \\n\\t{idx_gt_300}\")\n\nidx_lt_6 = np.where(N_UNIQUE_FRAMES < 6)\nprint(f\"\\n... #frames less than 6: \\n\\t{idx_lt_6}\")\n\n# Show frame","metadata":{"execution":{"iopub.status.busy":"2023-11-02T09:23:39.639094Z","iopub.execute_input":"2023-11-02T09:23:39.639381Z","iopub.status.idle":"2023-11-02T09:23:39.652327Z","shell.execute_reply.started":"2023-11-02T09:23:39.639358Z","shell.execute_reply":"2023-11-02T09:23:39.651460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## simple modelï¼©Preprocess","metadata":{}},{"cell_type":"markdown","source":"Preprocess\n- Use mean and std of coordinate(x, y, z) from lip(40), hands(21+21) and pose(33) as traing data.\n- dataset size (94477, 690)\n    - 690 is from (40+21+21+33)*3*2\n\nTraining\n- input size (N, 64, 690)\n- NN model\n    - liner\n    - droupout\n    - batch normalization\n    - activation function: gelu\n    - output: #class=250\n- corss validation: 5-fold\n- 40 epoch\n- loss fn.: CrossEntropy\n- optimizer: Adam with LearningRate scheduler","metadata":{}},{"cell_type":"code","source":"LIPS_IDXS = np.array([\n        61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n        291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n        78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n        95, 88, 178, 87, 14, 317, 402,318, 324, 308,\n    ])\n# left_hand: 468:489\n# pose: 489:522\n# right_hand: 522:543\n    \ndef create_features(row):\n    # whe using `train_df.values`\n    # path = row[0]\n    # sign = row[3]\n    \n    # whe using `train_df.iterrows()`\n    path = row[1].path\n    sign = row[1].sign\n    df = pd.read_parquet(get_full_path(path), columns=['x', 'y', 'z'])\n    df = df.fillna(0)\n    n_frames = int(df.shape[0]/543)\n    sample_arr = df.values.reshape(n_frames, 543, 3)\n\n    # (N, 40, 3)\n    lip = sample_arr[:, LIPS_IDXS, :]\n    left_hand = sample_arr[:, 468:489, :]\n    pose = sample_arr[:, 489:522, :]\n    right_hand = sample_arr[:, 522:, :]\n\n    # mean, std\n    lip_mean = lip.mean(axis=0).reshape(-1)\n    lip_std = lip.std(axis=0).reshape(-1)\n    left_hand_mean = left_hand.mean(axis=0).reshape(-1)\n    left_hand_std = left_hand.std(axis=0).reshape(-1)\n    pose_mean = pose.mean(axis=0).reshape(-1)\n    pose_std = pose.std(axis=0).reshape(-1)\n    right_hand_mean = right_hand.mean(axis=0).reshape(-1)\n    right_hand_std = right_hand.std(axis=0).reshape(-1)\n\n    # concate\n    ## lip, r_hand, l_hand, pose -> shape will be (2*40*3 + 2*21*3 + 2*21*3 + 2*33*3, ) == (690, )\n    features = np.concatenate((lip_mean, lip_std, left_hand_mean, left_hand_std, pose_mean, pose_std, right_hand_mean, right_hand_std))\n    \n    return features, dict.get(sign)\n\n# when using `train_df.iterrows()`, `row` is different from `pool.map()` so this can't work\n#\n# row = train_df.iloc[0]\n# features, sign = create_features(row)\n# print(features.shape)\n# print(sign)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T09:25:28.449630Z","iopub.execute_input":"2023-11-02T09:25:28.450345Z","iopub.status.idle":"2023-11-02T09:25:28.462638Z","shell.execute_reply.started":"2023-11-02T09:25:28.450315Z","shell.execute_reply":"2023-11-02T09:25:28.461503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n... train_df.iterrows()\n\nusing multiprocessing and pool.map() to apply a function to each row of a pandas DataFrame, \npool.map() passes each row as a tuple\n\"\"\"\n\nwith Pool() as pool:\n#     results = list(tqdm(pool.imap(create_features, train_df.values, chunksize=250)))\n    results = list(tqdm(pool.imap(create_features, train_df.iterrows(), chunksize=500), total=len(train_df)))\n    \ndata_X = np.array([res[0] for res in results])\ndata_y = np.array([res[1] for res in results])\n\nprint(data_X.shape)\nprint(data_y.shape)\n\n# for i, row in train_df[:10].iterrows():\n#     features, label = create_features(row)\n#     print(features.shape)\n#     print(label)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T09:25:33.422762Z","iopub.execute_input":"2023-11-02T09:25:33.423102Z","iopub.status.idle":"2023-11-02T09:32:52.389445Z","shell.execute_reply.started":"2023-11-02T09:25:33.423075Z","shell.execute_reply":"2023-11-02T09:32:52.388525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2023-11-02T09:34:08.841732Z","iopub.execute_input":"2023-11-02T09:34:08.842616Z","iopub.status.idle":"2023-11-02T09:34:13.501079Z","shell.execute_reply.started":"2023-11-02T09:34:08.842574Z","shell.execute_reply":"2023-11-02T09:34:13.500212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ASLData(Dataset):\n    def __init__(self, datax, datay):\n        self.datax = datax\n        self.datay = datay\n        \n    def __getitem__(self, index):\n        return self.datax[index,:], self.datay[index]\n        \n    def __len__(self):\n        return len(self.datay)\n\n    \nclass ASLModel(nn.Module):\n    def __init__(self, p):\n        super(ASLModel, self).__init__()\n        self.layer0 = nn.Linear(690, 2048)\n        self.bn0 = nn.BatchNorm1d(2048)  # Batch normalization layer\n        self.dropout = nn.Dropout(p=p)  # Dropout layer\n        self.layer1 = nn.Linear(2048, 1024)\n        self.bn1 = nn.BatchNorm1d(1024)\n        self.layer2 = nn.Linear(1024, 512)\n        self.bn2=nn.BatchNorm1d(512)\n        self.layer3=nn.Linear(512,500)\n        \n    def forward(self, x):\n        x = self.layer0(x)\n        x = self.bn0(x)\n        x = F.gelu(x)\n        x = self.dropout(x)\n        x = self.layer1(x)\n        x = self.bn1(x)\n        x = F.gelu(x)\n        x = self.dropout(x)\n        x = self.layer2(x)\n        x = F.gelu(x)\n        x = self.layer3(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-11-02T09:58:05.869661Z","iopub.execute_input":"2023-11-02T09:58:05.870556Z","iopub.status.idle":"2023-11-02T09:58:05.881030Z","shell.execute_reply.started":"2023-11-02T09:58:05.870523Z","shell.execute_reply":"2023-11-02T09:58:05.879848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\nEPOCHS = 50\nBATCH_SIZE = 128\nN_SPLITS = 5  # Number of cross-validation folds\n\ntrain_loss = np.zeros((N_SPLITS, EPOCHS))\ntrain_acc = np.zeros((N_SPLITS, EPOCHS))\nval_loss = np.zeros((N_SPLITS, EPOCHS))\nval_acc = np.zeros((N_SPLITS, EPOCHS))\n\nkf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n\n# Iterate over cross-validation folds\nfold = 0\nfor train_idx, val_idx in kf.split(data_X):\n    print(f\"\\n\\n... Training on Fold: {fold+1}\\n\")\n    trainx, valx = data_X[train_idx], data_X[val_idx]\n    trainy, valy = data_y[train_idx], data_y[val_idx]\n\n    train_data = ASLData(trainx, trainy)\n    val_data = ASLData(valx, valy)\n\n    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, num_workers=4, shuffle=True)\n    val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, num_workers=4, shuffle=False)\n\n    model = ASLModel(0.4).cuda()\n    opt = torch.optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n    sched = torch.optim.lr_scheduler.StepLR(opt, step_size=300, gamma=0.9)\n\n    for epoch in range(EPOCHS):\n        model.train()\n        train_loss_sum = 0.\n        train_correct = 0\n        train_total = 0\n\n        for x, y in train_loader:\n            x = torch.Tensor(x).float().cuda()\n            y = torch.Tensor(y).long().cuda()\n\n            y_pred = model(x)\n\n            loss = criterion(y_pred, y)\n            loss.backward()\n            opt.step()\n            opt.zero_grad()\n\n            train_loss_sum += loss.item()\n            train_correct += np.sum((np.argmax(y_pred.detach().cpu().numpy(), axis=1) == y.cpu().numpy()))\n            train_total += 1\n            sched.step()\n\n        val_loss_sum = 0.\n        val_correct = 0\n        val_total = 0\n\n        model.eval()\n        for x, y in val_loader:\n            x = torch.Tensor(x).float().cuda()\n            y = torch.Tensor(y).long().cuda()\n\n            with torch.no_grad():\n                y_pred = model(x)\n                loss = criterion(y_pred, y)\n                val_loss_sum += loss.item()\n                val_correct += np.sum((np.argmax(y_pred.cpu().numpy(), axis=1) == y.cpu().numpy()))\n                val_total += 1\n\n        train_loss[fold, epoch] = train_loss_sum / train_total\n        train_acc[fold, epoch] = train_correct / len(train_data)\n        val_loss[fold, epoch] = val_loss_sum / val_total\n        val_acc[fold, epoch] = val_correct / len(val_data)\n\n        print(f\"Epoch:{epoch} > Train Loss: {(train_loss_sum / train_total):.04f}, Train Acc: {train_correct / len(train_data):0.04f}\")\n        print(f\"Epoch:{epoch} > Val Loss: {(val_loss_sum / val_total):.04f}, Val Acc: {val_correct / len(val_data):0.04f}\")\n        print(\"=\" * 50)\n\n    fold += 1","metadata":{"execution":{"iopub.status.busy":"2023-11-02T09:59:58.806840Z","iopub.execute_input":"2023-11-02T09:59:58.807601Z","iopub.status.idle":"2023-11-02T10:17:50.805958Z","shell.execute_reply.started":"2023-11-02T09:59:58.807564Z","shell.execute_reply":"2023-11-02T10:17:50.804678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss and Accuracy","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Assuming N_SPLITS and EPOCHS have appropriate values\n\n# Calculate the mean and standard deviation of the loss across folds\nmean_train_loss = np.mean(train_loss, axis=0)\nmean_val_loss = np.mean(val_loss, axis=0)\nstd_train_loss = np.std(train_loss, axis=0)\nstd_val_loss = np.std(val_loss, axis=0)\n\n# Generate the x-axis values (epochs)\nepochs = range(1, EPOCHS + 1)\n\n# Plot the average loss curves with error bars\nplt.errorbar(epochs, mean_train_loss, yerr=std_train_loss, label='Train')\nplt.errorbar(epochs, mean_val_loss, yerr=std_val_loss, label='Validation')\n\n# Set plot labels and title\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss Curve (Cross-Validation)\")\nplt.legend()\n\n# Display the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-02T10:30:16.944083Z","iopub.execute_input":"2023-11-02T10:30:16.945335Z","iopub.status.idle":"2023-11-02T10:30:17.697690Z","shell.execute_reply.started":"2023-11-02T10:30:16.945285Z","shell.execute_reply":"2023-11-02T10:30:17.696704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming N_SPLITS and EPOCHS have appropriate values\n\n# Calculate the mean and standard deviation of the loss across folds\nmean_train_acc = np.mean(train_acc, axis=0)\nmean_val_acc = np.mean(val_acc, axis=0)\nstd_train_acc = np.std(train_acc, axis=0)\nstd_val_acc = np.std(val_acc, axis=0)\n\n# Generate the x-axis values (epochs)\nepochs = range(1, EPOCHS + 1)\n\n# Plot the average loss curves with error bars\nplt.errorbar(epochs, mean_train_acc, yerr=std_train_acc, label='Train')\nplt.errorbar(epochs, mean_val_acc, yerr=std_val_acc, label='Validation')\n\n# Set plot labels and title\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Acc\")\nplt.title(\"Acc Curve (Cross-Validation)\")\nplt.legend()\n\n# Display the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T10:30:25.038799Z","iopub.execute_input":"2023-11-02T10:30:25.039584Z","iopub.status.idle":"2023-11-02T10:30:25.342106Z","shell.execute_reply.started":"2023-11-02T10:30:25.039549Z","shell.execute_reply":"2023-11-02T10:30:25.341200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-06-01T05:15:57.391702Z","iopub.status.idle":"2023-06-01T05:15:57.392433Z","shell.execute_reply.started":"2023-06-01T05:15:57.392192Z","shell.execute_reply":"2023-06-01T05:15:57.392214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-06-01T05:15:57.39384Z","iopub.status.idle":"2023-06-01T05:15:57.394576Z","shell.execute_reply.started":"2023-06-01T05:15:57.394345Z","shell.execute_reply":"2023-06-01T05:15:57.394367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-06-01T05:15:57.395871Z","iopub.status.idle":"2023-06-01T05:15:57.396649Z","shell.execute_reply.started":"2023-06-01T05:15:57.396408Z","shell.execute_reply":"2023-06-01T05:15:57.396433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## old\n","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\n\n# Plotting loss\nsns.lineplot(train_loss, label='train_loss')\nsns.lineplot(val_loss, label='val_loss')\n\n# Set plot title and labels\nplt.title('Plot of train_loss and val_loss')\nplt.xlabel('epoch')\nplt.ylabel('loss')\n\n# Display the legend\nplt.legend()\n\n# Show the plot\nplt.show()\n\n\n# Plotting accuracy\nsns.lineplot(train_acc, label='train_acc')\nsns.lineplot(val_acc, label='val_acc')\n\n# Set plot title and labels\nplt.title('Plot of train_acc and val_acc')\nplt.xlabel('epoch')\nplt.ylabel('acc')\n\n# Display the legend\nplt.legend()\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-01T05:15:57.397987Z","iopub.status.idle":"2023-06-01T05:15:57.398727Z","shell.execute_reply.started":"2023-06-01T05:15:57.398483Z","shell.execute_reply":"2023-06-01T05:15:57.398504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}